<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://wanwenli.com/atom.xml" rel="self" type="application/atom+xml" /><link href="https://wanwenli.com/" rel="alternate" type="text/html" /><updated>2020-10-21T21:11:12+08:00</updated><id>https://wanwenli.com/atom.xml</id><title type="html">Salmon Says</title><subtitle>Stay curious and keep asking questions</subtitle><author><name>Wan Wenli</name></author><entry><title type="html">My open-source contributions</title><link href="https://wanwenli.com/life/2020/10/01/open-source-contributions.html" rel="alternate" type="text/html" title="My open-source contributions" /><published>2020-10-01T00:00:00+08:00</published><updated>2020-10-01T00:00:00+08:00</updated><id>https://wanwenli.com/life/2020/10/01/open-source-contributions</id><content type="html" xml:base="https://wanwenli.com/life/2020/10/01/open-source-contributions.html">&lt;h1 id=&quot;source-code&quot;&gt;Source code&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/petertodd/python-bitcoinlib/pull/177&quot;&gt;python-bitcoinlib&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/996icu/996.ICU/pull/25416&quot;&gt;996.ICU&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aws/aws-node-termination-handler/pull/259&quot;&gt;aws-node-termination-handler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;issues&quot;&gt;Issues&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/issues/856#issuecomment-663989442&quot;&gt;ingress-nginx&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wan Wenli</name></author><category term="open-source" /><category term="github" /><category term="technology" /><category term="programming" /><summary type="html">Source code python-bitcoinlib 996.ICU aws-node-termination-handler Issues ingress-nginx</summary></entry><entry><title type="html">CKA exam tips</title><link href="https://wanwenli.com/kubernetes/2020/05/10/cka-exam-tips.html" rel="alternate" type="text/html" title="CKA exam tips" /><published>2020-05-10T00:00:00+08:00</published><updated>2020-05-10T00:00:00+08:00</updated><id>https://wanwenli.com/kubernetes/2020/05/10/cka-exam-tips</id><content type="html" xml:base="https://wanwenli.com/kubernetes/2020/05/10/cka-exam-tips.html">&lt;p&gt;It’s simple.
Just make sure that you have taken the course on Udemy and another practice.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/&quot;&gt;Certified Kubernetes Administrator (CKA) with Practice Tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://killer.sh/&quot;&gt;Killer Shell - Kubernetes Exam Simulator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please complete all practice tests in the Udemy course.&lt;/p&gt;

&lt;p&gt;I feel that the difficulty of those practices in Killer Shell is higher than that of the actual exam,
so do &lt;em&gt;not&lt;/em&gt; panic if you cannot finish them on time.
Another good thing about Killer Shell is that
it gives you some really helpful tips on how to set up your exam environment.&lt;/p&gt;

&lt;p&gt;glhf&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="CKA" /><category term="k8s" /><summary type="html">It’s simple. Just make sure that you have taken the course on Udemy and another practice. Certified Kubernetes Administrator (CKA) with Practice Tests Killer Shell - Kubernetes Exam Simulator Please complete all practice tests in the Udemy course. I feel that the difficulty of those practices in Killer Shell is higher than that of the actual exam, so do not panic if you cannot finish them on time. Another good thing about Killer Shell is that it gives you some really helpful tips on how to set up your exam environment. glhf</summary></entry><entry><title type="html">Too many redirects when forcing SSL redirection on an ingress</title><link href="https://wanwenli.com/kubernetes/2019/05/29/k8s-ingress-force-ssl-redirect.html" rel="alternate" type="text/html" title="Too many redirects when forcing SSL redirection on an ingress" /><published>2019-05-29T00:00:00+08:00</published><updated>2019-05-29T00:00:00+08:00</updated><id>https://wanwenli.com/kubernetes/2019/05/29/k8s-ingress-force-ssl-redirect</id><content type="html" xml:base="https://wanwenli.com/kubernetes/2019/05/29/k8s-ingress-force-ssl-redirect.html">&lt;p&gt;In summary, these are the steps for SSL redirection to work properly and
this method has been mentioned by
&lt;a href=&quot;https://stackoverflow.com/a/64224737/1397473&quot;&gt;this answer on stackoverflow&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use-forwarded-headers: &quot;true&quot;&lt;/code&gt; into the ConfigMap of ingress controller&lt;/li&gt;
  &lt;li&gt;Do a rolling upgrade on the deployment of Nginx ingress controller,
so that the new value in the ConfigMap would be picked up.&lt;/li&gt;
  &lt;li&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;&lt;/code&gt; into the annotations of an ingress&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I found this by reading the source code of
&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx&quot;&gt;ingress-nginx&lt;/a&gt;.
Here is a detailed explanation on how it works.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TOO_MANY_REDIRECTS&lt;/code&gt; error happens because no matter HTTP or HTTPS is used,
Nginx controller always return
&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/308&quot;&gt;308 REDIRECT&lt;/a&gt;
to clients.
Next, the browser goes into an infinite loop of redirections.
Why? In the following section it will be explained.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;force-ssl-redirect&lt;/code&gt; annotation is set to true on an ingress level,
Nginx ingress controller adds/updates the following snippet into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx.conf&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-lua&quot; data-lang=&quot;lua&quot;&gt;&lt;span class=&quot;n&quot;&gt;rewrite_by_lua_block&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lua_ingress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;force_ssl_redirect&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;use_port_in_redirects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;balancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note: when SSL redirect is false in an ingress, the value of force_ssl_redirect is also false.&lt;/p&gt;

&lt;p&gt;These are the a few code snippets you will need to look into to understand how this fix works.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the source code of ingress-nginx,
&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/blob/bf11e2ef636cd535b66ebb2ab638a941662da699/rootfs/etc/nginx/lua/lua_ingress.lua#L122&quot;&gt;here&lt;/a&gt;
defines how the Lua block in nginx.conf works.&lt;/li&gt;
  &lt;li&gt;When force_ssl_redirect is true, function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redirect_to_https&lt;/code&gt; alone determines whether the request should be redirected.&lt;/li&gt;
  &lt;li&gt;According to the implementation of
&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/blob/bf11e2ef636cd535b66ebb2ab638a941662da699/rootfs/etc/nginx/lua/lua_ingress.lua#L57&quot;&gt;redirect_to_https&lt;/a&gt;,
its return value solely depends on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngx.var.pass_access_scheme&lt;/code&gt;,
because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngx.var.scheme&lt;/code&gt; is either http or https. Later I will argue that it has always been http.&lt;/li&gt;
  &lt;li&gt;According
&lt;a href=&quot;https://github.com/kubernetes/ingress-nginx/blob/bf11e2ef636cd535b66ebb2ab638a941662da699/rootfs/etc/nginx/lua/lua_ingress.lua#L97&quot;&gt;this chunk&lt;/a&gt;,
it takes the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_x_forwarded_proto&lt;/code&gt; if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use-forwarded-headers&lt;/code&gt; is true and
that’s why it is enabled in ingress controller.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since the following snippet works as a hacky way,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http_x_forwarded_proto&lt;/code&gt; must reflect the true protocol of the request.&lt;/p&gt;

&lt;div class=&quot;language-nginx highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$http_x_forwarded_proto&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'https')&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;301&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$host$request_uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Q.E.D.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="kubernetes" /><category term="k8s" /><category term="nginx-ingress-controller" /><summary type="html">In summary, these are the steps for SSL redirection to work properly and this method has been mentioned by this answer on stackoverflow. Add use-forwarded-headers: &quot;true&quot; into the ConfigMap of ingress controller Do a rolling upgrade on the deployment of Nginx ingress controller, so that the new value in the ConfigMap would be picked up. Add nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot; into the annotations of an ingress I found this by reading the source code of ingress-nginx. Here is a detailed explanation on how it works. The TOO_MANY_REDIRECTS error happens because no matter HTTP or HTTPS is used, Nginx controller always return 308 REDIRECT to clients. Next, the browser goes into an infinite loop of redirections. Why? In the following section it will be explained. When force-ssl-redirect annotation is set to true on an ingress level, Nginx ingress controller adds/updates the following snippet into nginx.conf. rewrite_by_lua_block { lua_ingress.rewrite({ force_ssl_redirect = true, use_port_in_redirects = false, }) balancer.rewrite() plugins.run() } Note: when SSL redirect is false in an ingress, the value of force_ssl_redirect is also false. These are the a few code snippets you will need to look into to understand how this fix works. In the source code of ingress-nginx, here defines how the Lua block in nginx.conf works. When force_ssl_redirect is true, function redirect_to_https alone determines whether the request should be redirected. According to the implementation of redirect_to_https, its return value solely depends on ngx.var.pass_access_scheme, because ngx.var.scheme is either http or https. Later I will argue that it has always been http. According this chunk, it takes the value of http_x_forwarded_proto if use-forwarded-headers is true and that’s why it is enabled in ingress controller. Since the following snippet works as a hacky way, http_x_forwarded_proto must reflect the true protocol of the request. if ($http_x_forwarded_proto != 'https') { return 301 https://$host$request_uri; } Q.E.D.</summary></entry><entry><title type="html">How do Bitcoin mining pools pay their miners?</title><link href="https://wanwenli.com/blockchain/2018/09/13/How-do-bitcoin-mining-pools-pay.html" rel="alternate" type="text/html" title="How do Bitcoin mining pools pay their miners?" /><published>2018-09-13T00:00:00+08:00</published><updated>2018-09-13T00:00:00+08:00</updated><id>https://wanwenli.com/blockchain/2018/09/13/How-do-bitcoin-mining-pools-pay</id><content type="html" xml:base="https://wanwenli.com/blockchain/2018/09/13/How-do-bitcoin-mining-pools-pay.html">&lt;p&gt;Since the price of Bitcoin surged in the 2017 Q4,
many mining pools have surfaced.
Over the globe there are over 20 active and recognizable mining pools.
You can find more pools and their addresses from
&lt;a href=&quot;https://github.com/btccom/Blockchain-Known-Pools/blob/master/pools.json&quot;&gt;this GitHub repo&lt;/a&gt;.
This article explains a particular type of routine job in a Bitcoin mining pool:
how does it pay Bitcoins to its miners?
Actually you will find out that it might not be the same as you thought.&lt;/p&gt;

&lt;h3 id=&quot;directly-from-coinbase-addresses&quot;&gt;Directly from coinbase addresses&lt;/h3&gt;

&lt;p&gt;This is the simplest way.
SlushPool has been using this method for some time.
You can easily find payout transactions miners from address
&lt;a href=&quot;https://btc.com/1CK6KHY6MHgYvmRQ4PAafKYDrg1ejbH1cE&quot;&gt;1CK6KHY6MHgYvmRQ4PAafKYDrg1ejbH1cE&lt;/a&gt;
which is the publicly known coinbase address of SlushPool.
On a typical day you can observe more than five transactions from this address
each of which is with a huge number of outputs to various addresses -
it is the typical way of identifying payments from a mining pool to its miners.&lt;/p&gt;

&lt;p&gt;Other pools that also use this method are
BTC.TOP and Huobi Pool.
However, you can find that BTC.TOP also uses intermediate addresses to pay its miners.&lt;/p&gt;

&lt;h3 id=&quot;pay-via-intermediate-wallets&quot;&gt;Pay via intermediate wallets&lt;/h3&gt;

&lt;p&gt;This is probably the most common practice of pools.
Take &lt;a href=&quot;https://btc.com&quot;&gt;BTC.com&lt;/a&gt; for example,
you can find its coinbase address is
&lt;a href=&quot;https://btc.com/bc1qjl8uwezzlech723lpnyuza0h2cdkvxvh54v3dn&quot;&gt;bc1qjl8uwezzlech723lpnyuza0h2cdkvxvh54v3dn&lt;/a&gt;.
There are many transactions associated with this address,
most of which are coinbase transactions.
However there are some Bitcoins sent by it to other addresses,
for example,
&lt;a href=&quot;https://btc.com/3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2&quot;&gt;3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2&lt;/a&gt;
in transaction
&lt;a href=&quot;https://btc.com/d094eca893253c12bb53f55ed834281349eda9e59505fa91d07e114fd616aa02&quot;&gt;d094eca893253c12bb53f55ed834281349eda9e59505fa91d07e114fd616aa02&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Therefore,
&lt;a href=&quot;https://btc.com/3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2&quot;&gt;3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2&lt;/a&gt;
is one of the intermediate addresses used by BTC.com.
Every day there are a few transactions from it to many addresses,
probably thousands of outputs in a single transaction,
and this is how BTC.com pays its miners.&lt;/p&gt;

&lt;h3 id=&quot;antpool-payments-chain&quot;&gt;AntPool: payments chain&lt;/h3&gt;

&lt;p&gt;AntPool is a very special case and
its payment pattern is the most difficult to trace.
The whole process of payments is like a chain or loop.
Almost every day it starts from in the morning
and last until late afternoon.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate a new wallet&lt;/li&gt;
  &lt;li&gt;Use the intermediate address to pay 101 addresses&lt;/li&gt;
  &lt;li&gt;Among the 101 recipients 100 are miners,
while the one newly generated wallet receives &lt;em&gt;all&lt;/em&gt; the change&lt;/li&gt;
  &lt;li&gt;Use the balance in the change wallet to pay another 100 miners + 1 new wallet&lt;/li&gt;
  &lt;li&gt;Repeat the process above until all miner addresses are paid&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The end result is that all the intermediate change wallets are empty and
they will &lt;em&gt;not&lt;/em&gt; be reused.
Only the very last change wallet has some Bitcoins in it.
Personally I like this process because it gives the miners highest level of
privacy.
If you wish to find out all miners’ addresses under AntPool by hands,
you have to traverse many transactions.
But for other pools, you only need to checkout less than ten transactions,
probably even less than five.&lt;/p&gt;

&lt;h3 id=&quot;f2pool-pay-without-transaction-fees&quot;&gt;F2Pool: pay without transaction fees&lt;/h3&gt;

&lt;p&gt;Everyday F2Pool broadcasts a transaction with zero transaction fee
to pay its miners.
After that, F2Pool tries to mine it because
no other miner is willing to mine a transaction without any fee.
This approach saves money, apparently.
A major disadvantage of it is there are huge
uncertainties in the time when its miners receive Bitcoins.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="blockchain" /><category term="bitcoin" /><summary type="html">Since the price of Bitcoin surged in the 2017 Q4, many mining pools have surfaced. Over the globe there are over 20 active and recognizable mining pools. You can find more pools and their addresses from this GitHub repo. This article explains a particular type of routine job in a Bitcoin mining pool: how does it pay Bitcoins to its miners? Actually you will find out that it might not be the same as you thought. Directly from coinbase addresses This is the simplest way. SlushPool has been using this method for some time. You can easily find payout transactions miners from address 1CK6KHY6MHgYvmRQ4PAafKYDrg1ejbH1cE which is the publicly known coinbase address of SlushPool. On a typical day you can observe more than five transactions from this address each of which is with a huge number of outputs to various addresses - it is the typical way of identifying payments from a mining pool to its miners. Other pools that also use this method are BTC.TOP and Huobi Pool. However, you can find that BTC.TOP also uses intermediate addresses to pay its miners. Pay via intermediate wallets This is probably the most common practice of pools. Take BTC.com for example, you can find its coinbase address is bc1qjl8uwezzlech723lpnyuza0h2cdkvxvh54v3dn. There are many transactions associated with this address, most of which are coinbase transactions. However there are some Bitcoins sent by it to other addresses, for example, 3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2 in transaction d094eca893253c12bb53f55ed834281349eda9e59505fa91d07e114fd616aa02. Therefore, 3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2 is one of the intermediate addresses used by BTC.com. Every day there are a few transactions from it to many addresses, probably thousands of outputs in a single transaction, and this is how BTC.com pays its miners. AntPool: payments chain AntPool is a very special case and its payment pattern is the most difficult to trace. The whole process of payments is like a chain or loop. Almost every day it starts from in the morning and last until late afternoon. Generate a new wallet Use the intermediate address to pay 101 addresses Among the 101 recipients 100 are miners, while the one newly generated wallet receives all the change Use the balance in the change wallet to pay another 100 miners + 1 new wallet Repeat the process above until all miner addresses are paid The end result is that all the intermediate change wallets are empty and they will not be reused. Only the very last change wallet has some Bitcoins in it. Personally I like this process because it gives the miners highest level of privacy. If you wish to find out all miners’ addresses under AntPool by hands, you have to traverse many transactions. But for other pools, you only need to checkout less than ten transactions, probably even less than five. F2Pool: pay without transaction fees Everyday F2Pool broadcasts a transaction with zero transaction fee to pay its miners. After that, F2Pool tries to mine it because no other miner is willing to mine a transaction without any fee. This approach saves money, apparently. A major disadvantage of it is there are huge uncertainties in the time when its miners receive Bitcoins.</summary></entry><entry><title type="html">A brief guide to Bitcoin lightning network with examples</title><link href="https://wanwenli.com/blockchain/2018/06/28/Bitcoin-lightning-network.html" rel="alternate" type="text/html" title="A brief guide to Bitcoin lightning network with examples" /><published>2018-06-28T00:00:00+08:00</published><updated>2018-06-28T00:00:00+08:00</updated><id>https://wanwenli.com/blockchain/2018/06/28/Bitcoin-lightning-network</id><content type="html" xml:base="https://wanwenli.com/blockchain/2018/06/28/Bitcoin-lightning-network.html">&lt;p&gt;Why another post about the lightning network (LN) of Bitcoin?&lt;/p&gt;

&lt;p&gt;The primary reason is that
some of the resources online are &lt;em&gt;not&lt;/em&gt; up to date.
The technological details of LN are not intuitive enough
and not easy to grasp.
Without examples, confusion and misunderstanding may arise.&lt;/p&gt;

&lt;p&gt;If you are a technician, researcher, engineer,
blockchain hobbyist or enthusiast
who already has some knowledge about LN,
this article is for you.
Do not worry if you know nothing about LN,
because this article leads you to the most useful resources
and tell you which piece of information is accurate in them.
Some understanding about
&lt;a href=&quot;https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki&quot;&gt;segregated witness&lt;/a&gt;
is strongly recommended before
reading the remaining part of this post,
as current LN payments heavily rely on it.&lt;/p&gt;

&lt;p&gt;In summary this post focuses on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;what are the reliable resources to read/watch in order to understand LN;&lt;/li&gt;
  &lt;li&gt;what info inside these articles and videos is accurate and what is outdated;&lt;/li&gt;
  &lt;li&gt;how LN works, roughly;&lt;/li&gt;
  &lt;li&gt;how to find LN transactions on-chain.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, this article does &lt;em&gt;not&lt;/em&gt; discuss
how lightning network will affect bitcoin ecosystem,
its advantages and drawbacks,
how much transaction rate will improve, or even coin price!&lt;/p&gt;

&lt;p&gt;Good news: this article provides &lt;strong&gt;concrete examples&lt;/strong&gt; of payments via LN
that have been permanently written on Bitcoin blockchain!
So far I have not seen any other articles that give out examples.&lt;/p&gt;

&lt;p&gt;Last but not the least,
this post is my personal humble understanding on LN.
I am open for comments and critics.&lt;/p&gt;

&lt;h3 id=&quot;articles-and-videos&quot;&gt;Articles and videos&lt;/h3&gt;

&lt;p&gt;The GitHub repo
&lt;a href=&quot;https://github.com/bcongdon/awesome-lightning-network&quot;&gt;awesome-lightning-network&lt;/a&gt;
lists out many resources for LN.
It is a good starting point.
However in my point of view,
there is only one that is sufficiently comprehensive: the
&lt;a href=&quot;https://lightning.network/lightning-network-paper.pdf&quot;&gt;lightning network paper&lt;/a&gt;.
I strongly recommend you to read through it.&lt;/p&gt;

&lt;p&gt;You may have come across
&lt;a href=&quot;https://www.youtube.com/watch?v=8zVzw912wPo&amp;amp;t=2317s&quot;&gt;this YouTube video&lt;/a&gt;
on LN.
It appears as the 2&lt;sup&gt;nd&lt;/sup&gt; search result of
“lightning network” on YouTube.
Its weakness is that for bi-directional payments,
how to penalise an old commitment transaction and
revoke all the funds is not clear.
To understand the details of penalty
which is an essential part to ensure the safety and trustworthiness of LN,
please read the LN paper.
Penalty on HTLC transactions follows a very similar mechanism.
Later you will see both successfully claimed and penalised fund on-chain.&lt;/p&gt;

&lt;p&gt;Besides, the articles and videos share another fatal issue:
the scripts illustrated in them &lt;em&gt;cannot&lt;/em&gt; be found on the most recent blocks.
For example, I am &lt;em&gt;unable&lt;/em&gt; to find an input script
that follow the format below which is on page 31 of the LN paper
and implemented in a JavaScript LN protocol,
&lt;a href=&quot;https://github.com/yoursnetwork/yours-channels&quot;&gt;yours-channels&lt;/a&gt;.
However if you do find one, please leave your comment.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OP IF
  OP HASH160 &amp;lt;Hash160 (R)&amp;gt; OP EQUALVERIFY
  2 &amp;lt;Ali c e 2&amp;gt; &amp;lt;Bob2&amp;gt; OP CHECKMULTISIG
OP ELSE
  2 &amp;lt;Ali c e 1&amp;gt; &amp;lt;Bob1&amp;gt; OP CHECKMULTISIG
OP ENDIF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I suppose it is because
the specifications of LN have been updated and redefined in
&lt;a href=&quot;https://github.com/lightningnetwork/lightning-rfc&quot;&gt;this repo&lt;/a&gt;
called Basis of Lightning Technology (BOLT).
To be more specific, transactions and scripts are documented in
&lt;a href=&quot;https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md&quot;&gt;BOLT #3&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;source-code&quot;&gt;Source code&lt;/h3&gt;

&lt;p&gt;There is a handful of implementations for LN.
The most popular two are the
&lt;a href=&quot;https://github.com/lightningnetwork/lnd&quot;&gt;LND&lt;/a&gt; written in Go
and
&lt;a href=&quot;https://github.com/ElementsProject/lightning&quot;&gt;lightning&lt;/a&gt; in C.
I have found these two repos after encountering the
&lt;a href=&quot;https://lnmainnet.gaben.win/&quot;&gt;mainnet LN explorer&lt;/a&gt;
which gets readings from them.&lt;/p&gt;

&lt;p&gt;Both of them strictly follow the
&lt;a href=&quot;https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md&quot;&gt;BOLT #3&lt;/a&gt;
to build transactions and scripts.
Read more detailed source code here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lightningnetwork/lnd/blob/master/lnwallet/script_utils.go&quot;&gt;script_utils.go&lt;/a&gt;
in LND&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ElementsProject/lightning/blob/master/bitcoin/script.c&quot;&gt;script.c&lt;/a&gt; in lightning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you will see soon,
BOLT #3 is the dominantly (probably the only) observable form on-chain.&lt;/p&gt;

&lt;h3 id=&quot;how-to-find-a-ln-transaction&quot;&gt;How to find a LN transaction&lt;/h3&gt;

&lt;p&gt;There are two major forms of transactions that are surely from LN:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2-of-2-MULTISIG embedded in P2WSH followed by
a conditional time locked input (for bi-directional channels)&lt;/li&gt;
  &lt;li&gt;2-of-2-MULTISIG embedded in P2WSH followed by at least one HTLC input&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first one is the dominant type of input script in LN and
takes up more than 80% of transaction inputs broadcast by LN.
Later you will see that the second form has two versions,
one for sender and the other for receiver.&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;

&lt;p&gt;Here are some example hashes of LN transactions.&lt;/p&gt;

&lt;h5 id=&quot;to_local&quot;&gt;to_local&lt;/h5&gt;

&lt;p&gt;It is used to close a bi-directional channel and is
the most commonly seen form, as mentioned before.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;succeeded:
&lt;a href=&quot;https://btc.com/0191535bfda21f5dfec1c904775c5e2fbee8a985815c88d77258a0b42dba3526#in_0&quot;&gt;0191535bfda21f5dfec1c904775c5e2fbee8a985815c88d77258a0b42dba3526&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;penalised:
&lt;a href=&quot;https://btc.com/0da5e5dba5e793d50820c2275dab74912b121c8b7e34ce32a9dbfd4567a9bf8e#in_0&quot;&gt;0da5e5dba5e793d50820c2275dab74912b121c8b7e34ce32a9dbfd4567a9bf8e&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us examine the successful case:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Its upstream input is a 2-of-2 MULTISIG embedded in P2WSH,
the funding transaction.&lt;/li&gt;
  &lt;li&gt;The funding transaction has two outputs (commitment transactions)
and the other is a P2WPKH.
The P2WPKH is not time locked and
has been sent to the counterparty of the channel.&lt;/li&gt;
  &lt;li&gt;Its input witness starts with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;sig&amp;gt; 0&lt;/code&gt; form
therefore its fund has already been successfully taken.
According to the LN paper, the LN channel is &lt;em&gt;fully closed&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why the second input has been penalised?
Because there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt; right after the signature and
it executes the if-branch which revokes all the funds in the channel.
Both inputs were from the two outputs of
the previous funding transaction and there is only one output.
Therefore we can conclude that it is in a penalty transaction.&lt;/p&gt;

&lt;h5 id=&quot;sender-script&quot;&gt;sender script&lt;/h5&gt;

&lt;p&gt;Here are two examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;timeout:
&lt;a href=&quot;https://btc.com/a16f6d78a58d31fe7459887adf5bd6b4dd95277ea375d250c700cde9fa908bdb&quot;&gt;a16f6d78a58d31fe7459887adf5bd6b4dd95277ea375d250c700cde9fa908bdb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;claimed by preimage:
&lt;a href=&quot;https://btc.com/89c744f0806a57a9b4634c320703cc941aaf272f290296373b709499064335e5&quot;&gt;89c744f0806a57a9b4634c320703cc941aaf272f290296373b709499064335e5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The expenditures by timeout are easy to identify, as its format is simply
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0 &amp;lt;sig&amp;gt; &amp;lt;sig&amp;gt; 0&lt;/code&gt;.
How to identify if a HTLC transaction has used the preimage?
Here I would like to recommend the
&lt;a href=&quot;https://github.com/petertodd/python-bitcoinlib&quot;&gt;python bitcoin library&lt;/a&gt;.
I like it a lot because python provides power interactive terminal
and it is convenient to analyse bitcoin blockchain.&lt;/p&gt;

&lt;p&gt;Let us see the transaction input whose hash starts with “89c744”.
Its witness is in the form of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sig [unknown] scriptPubKey&lt;/code&gt;.
Later we will find out that the unknown part is
actually the hash160 of a secret, the hash that locks the contract.
A revocation script will appear in the exactly same form
but we shall see that the script does not execute the revocation path.&lt;/p&gt;

&lt;p&gt;The scriptPubKey part, according to BOLT #3, is decoded as&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bitcoin.core&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CScript&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CScript&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bytearray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromhex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'76a9149d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd38763ac672102aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c865977c820120876475527c210362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a52ae67a9146e3bef3f86aed6d6f825f13d1fa070039c866c5c88ac6868'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CScript&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OP_DUP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_HASH160&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'9d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_EQUAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_IF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_CHECKSIG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_ELSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'02aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c86597'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_SWAP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'20'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_EQUAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_NOTIF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_DROP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_SWAP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_CHECKMULTISIG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_ELSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_HASH160&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'6e3bef3f86aed6d6f825f13d1fa070039c866c5c'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_EQUALVERIFY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_CHECKSIG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_ENDIF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OP_ENDIF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, let us see the hash value of the unknown part,&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bitcoin&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CScript&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bitcoin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hash160&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bytearray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromhex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ae626cc4d6c208bdb3179b9d3efc7ae61779a9924b3852f01d0024afa84a4bbb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CScript&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'6e3bef3f86aed6d6f825f13d1fa070039c866c5c'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See? The hash160 of the previously unknown field is
equal to the value right after the last &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OP_HASH160&lt;/code&gt;
and before the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OP_EQUALVERIFY&lt;/code&gt;.
Therefore the fund is indeed taken by providing the preimage,
according to the HTLC script for senders defined in BOLT #3,
instead of being revoked.&lt;/p&gt;

&lt;p&gt;Here I am not going to show how the script should be executed
step by step on a stack.
You may do this yourself and keep in mind that
the exection path in a if-else branch is determined by a digit
whose value is either 1 or 0.&lt;/p&gt;

&lt;h5 id=&quot;receiver-script&quot;&gt;receiver script&lt;/h5&gt;

&lt;p&gt;Similarly, here are another two examples for receiver script:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;claimed by preimage:
&lt;a href=&quot;https://btc.com/36b1aff2ad0076be95b1ee1dc4036374998760c80c6583a6478a699e86658ac0&quot;&gt;36b1aff2ad0076be95b1ee1dc4036374998760c80c6583a6478a699e86658ac0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;timeout:
&lt;a href=&quot;https://btc.com/f9af9b93d66c7e5ee7dcbe0b53faa3d17aa6b9f4cc5b19f0985917b57d82c59a#in_0&quot;&gt;f9af9b93d66c7e5ee7dcbe0b53faa3d17aa6b9f4cc5b19f0985917b57d82c59a&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, if you find HTLC inputs that are penalised,
be sure to leave a comment.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;With all the resources presented by this article
and on-chain examples,
I hope you can better understand transactions in LN.&lt;/p&gt;

&lt;p&gt;Long live blockchain.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="blockchain" /><category term="bitcoin" /><summary type="html">Why another post about the lightning network (LN) of Bitcoin? The primary reason is that some of the resources online are not up to date. The technological details of LN are not intuitive enough and not easy to grasp. Without examples, confusion and misunderstanding may arise. If you are a technician, researcher, engineer, blockchain hobbyist or enthusiast who already has some knowledge about LN, this article is for you. Do not worry if you know nothing about LN, because this article leads you to the most useful resources and tell you which piece of information is accurate in them. Some understanding about segregated witness is strongly recommended before reading the remaining part of this post, as current LN payments heavily rely on it. In summary this post focuses on: what are the reliable resources to read/watch in order to understand LN; what info inside these articles and videos is accurate and what is outdated; how LN works, roughly; how to find LN transactions on-chain. Unfortunately, this article does not discuss how lightning network will affect bitcoin ecosystem, its advantages and drawbacks, how much transaction rate will improve, or even coin price! Good news: this article provides concrete examples of payments via LN that have been permanently written on Bitcoin blockchain! So far I have not seen any other articles that give out examples. Last but not the least, this post is my personal humble understanding on LN. I am open for comments and critics. Articles and videos The GitHub repo awesome-lightning-network lists out many resources for LN. It is a good starting point. However in my point of view, there is only one that is sufficiently comprehensive: the lightning network paper. I strongly recommend you to read through it. You may have come across this YouTube video on LN. It appears as the 2nd search result of “lightning network” on YouTube. Its weakness is that for bi-directional payments, how to penalise an old commitment transaction and revoke all the funds is not clear. To understand the details of penalty which is an essential part to ensure the safety and trustworthiness of LN, please read the LN paper. Penalty on HTLC transactions follows a very similar mechanism. Later you will see both successfully claimed and penalised fund on-chain. Besides, the articles and videos share another fatal issue: the scripts illustrated in them cannot be found on the most recent blocks. For example, I am unable to find an input script that follow the format below which is on page 31 of the LN paper and implemented in a JavaScript LN protocol, yours-channels. However if you do find one, please leave your comment. OP IF OP HASH160 &amp;lt;Hash160 (R)&amp;gt; OP EQUALVERIFY 2 &amp;lt;Ali c e 2&amp;gt; &amp;lt;Bob2&amp;gt; OP CHECKMULTISIG OP ELSE 2 &amp;lt;Ali c e 1&amp;gt; &amp;lt;Bob1&amp;gt; OP CHECKMULTISIG OP ENDIF I suppose it is because the specifications of LN have been updated and redefined in this repo called Basis of Lightning Technology (BOLT). To be more specific, transactions and scripts are documented in BOLT #3. Source code There is a handful of implementations for LN. The most popular two are the LND written in Go and lightning in C. I have found these two repos after encountering the mainnet LN explorer which gets readings from them. Both of them strictly follow the BOLT #3 to build transactions and scripts. Read more detailed source code here: script_utils.go in LND script.c in lightning As you will see soon, BOLT #3 is the dominantly (probably the only) observable form on-chain. How to find a LN transaction There are two major forms of transactions that are surely from LN: 2-of-2-MULTISIG embedded in P2WSH followed by a conditional time locked input (for bi-directional channels) 2-of-2-MULTISIG embedded in P2WSH followed by at least one HTLC input The first one is the dominant type of input script in LN and takes up more than 80% of transaction inputs broadcast by LN. Later you will see that the second form has two versions, one for sender and the other for receiver. Examples Here are some example hashes of LN transactions. to_local It is used to close a bi-directional channel and is the most commonly seen form, as mentioned before. succeeded: 0191535bfda21f5dfec1c904775c5e2fbee8a985815c88d77258a0b42dba3526 penalised: 0da5e5dba5e793d50820c2275dab74912b121c8b7e34ce32a9dbfd4567a9bf8e Let us examine the successful case: Its upstream input is a 2-of-2 MULTISIG embedded in P2WSH, the funding transaction. The funding transaction has two outputs (commitment transactions) and the other is a P2WPKH. The P2WPKH is not time locked and has been sent to the counterparty of the channel. Its input witness starts with &amp;lt;sig&amp;gt; 0 form therefore its fund has already been successfully taken. According to the LN paper, the LN channel is fully closed. Why the second input has been penalised? Because there is a 1 right after the signature and it executes the if-branch which revokes all the funds in the channel. Both inputs were from the two outputs of the previous funding transaction and there is only one output. Therefore we can conclude that it is in a penalty transaction. sender script Here are two examples: timeout: a16f6d78a58d31fe7459887adf5bd6b4dd95277ea375d250c700cde9fa908bdb claimed by preimage: 89c744f0806a57a9b4634c320703cc941aaf272f290296373b709499064335e5 The expenditures by timeout are easy to identify, as its format is simply 0 &amp;lt;sig&amp;gt; &amp;lt;sig&amp;gt; 0. How to identify if a HTLC transaction has used the preimage? Here I would like to recommend the python bitcoin library. I like it a lot because python provides power interactive terminal and it is convenient to analyse bitcoin blockchain. Let us see the transaction input whose hash starts with “89c744”. Its witness is in the form of sig [unknown] scriptPubKey. Later we will find out that the unknown part is actually the hash160 of a secret, the hash that locks the contract. A revocation script will appear in the exactly same form but we shall see that the script does not execute the revocation path. The scriptPubKey part, according to BOLT #3, is decoded as In [1]: from bitcoin.core import CScript In [2]: CScript(bytearray.fromhex('76a9149d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd38763ac672102aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c865977c820120876475527c210362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a52ae67a9146e3bef3f86aed6d6f825f13d1fa070039c866c5c88ac6868')) Out[2]: CScript([OP_DUP, OP_HASH160, x('9d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd3'), OP_EQUAL, OP_IF, OP_CHECKSIG, OP_ELSE, x('02aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c86597'), OP_SWAP, OP_SIZE, x('20'), OP_EQUAL, OP_NOTIF, OP_DROP, 2, OP_SWAP, x('0362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a'), 2, OP_CHECKMULTISIG, OP_ELSE, OP_HASH160, x('6e3bef3f86aed6d6f825f13d1fa070039c866c5c'), OP_EQUALVERIFY, OP_CHECKSIG, OP_ENDIF, OP_ENDIF]) Next, let us see the hash value of the unknown part, In [1]: import bitcoin In [2]: CScript([bitcoin.core.Hash160(bytearray.fromhex('ae626cc4d6c208bdb3179b9d3efc7ae61779a9924b3852f01d0024afa84a4bbb'))]) Out[2]: CScript([x('6e3bef3f86aed6d6f825f13d1fa070039c866c5c')]) See? The hash160 of the previously unknown field is equal to the value right after the last OP_HASH160 and before the OP_EQUALVERIFY. Therefore the fund is indeed taken by providing the preimage, according to the HTLC script for senders defined in BOLT #3, instead of being revoked. Here I am not going to show how the script should be executed step by step on a stack. You may do this yourself and keep in mind that the exection path in a if-else branch is determined by a digit whose value is either 1 or 0. receiver script Similarly, here are another two examples for receiver script: claimed by preimage: 36b1aff2ad0076be95b1ee1dc4036374998760c80c6583a6478a699e86658ac0 timeout: f9af9b93d66c7e5ee7dcbe0b53faa3d17aa6b9f4cc5b19f0985917b57d82c59a Again, if you find HTLC inputs that are penalised, be sure to leave a comment. Summary With all the resources presented by this article and on-chain examples, I hope you can better understand transactions in LN. Long live blockchain.</summary></entry><entry><title type="html">Apache Spark’s number of cores rethought</title><link href="https://wanwenli.com/spark/2018/04/02/spark-executor-cores.html" rel="alternate" type="text/html" title="Apache Spark's number of cores rethought" /><published>2018-04-02T00:00:00+08:00</published><updated>2018-04-02T00:00:00+08:00</updated><id>https://wanwenli.com/spark/2018/04/02/spark-executor-cores</id><content type="html" xml:base="https://wanwenli.com/spark/2018/04/02/spark-executor-cores.html">&lt;p&gt;There are tons of articles talking about setting the number of executors and cores for &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; applications.
Hence I am &lt;em&gt;not&lt;/em&gt; going to discuss anything about tuning these 2 parameters.
Instead I would like to address what does the values of these parameters mean
for nodes or containers which are executing your tasks.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The number of cores has no direct relation to physical CPU cores,
instead it is a logical counter
which determines the number of concurrent
tasks that are able to run on one executor.
I believe that many articles you can find have already addressed it.
However from the perspective of servers or containers
that receive tasks from the driver process,
the number of cores is able to limit the total number of tasks running
on one server/node/container.&lt;/p&gt;

&lt;h3 id=&quot;standalone-mode&quot;&gt;Standalone mode&lt;/h3&gt;

&lt;p&gt;Standalone mode is very easy to setup but has few features in
scheduling and access control.
Though I have touched both standalone and YARN mode,
starting from standalone mode is a good choice
because the implementation details are easy to inspect, compared to YARN mode
where lots of details are handled by YARN.&lt;/p&gt;

&lt;p&gt;Let’s start from worker process first.
It is recommended that each server only runs one single worker process
and I am going to treat it as an assumption.&lt;/p&gt;

&lt;p&gt;What are workers and executors?&lt;/p&gt;

&lt;p&gt;Each worker is Java process and so is an executor.
If the number of cores is not specified when starting a worker,
the worker will pick the number of physical cores from OS
to be its capacity of cores.
For Spark applications that demand executors,
worker processes spawn executor processes as responses to such requests.
Upon each creation of an executor process,
worker deduct the amount of memory and cores from the total value
it controls.
In &lt;a href=&quot;&quot;&gt;Worker&lt;/a&gt;
class, you can find in method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launchExecutor&lt;/code&gt; that
for each launch of a new executor, the count of used cores is incremented.
Therefore as I said,
the number of memory and cores are more like
&lt;em&gt;logical&lt;/em&gt; values rather than the actual amount of resources each executor uses.&lt;/p&gt;

&lt;p&gt;What does it look like in deeper details?
The number of memory (controlled by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark.executor.memory&lt;/code&gt;)
determines the &lt;strong&gt;maximum heap size&lt;/strong&gt; of executor process,
by forming the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Xmx&lt;/code&gt; parameter.
For example if you pass in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--conf spark.executor.memory=10240m&lt;/code&gt;
when submitting an Spark application,
the executor Java process will include a command line argument as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Xmx 10240m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala&quot;&gt;ExecutorRunner&lt;/a&gt;
is the wrapper class around each executor process.
It has an attribute which is an instance of Java
&lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/lang/Process.html&quot;&gt;Process&lt;/a&gt;.
Actually &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala&quot;&gt;CoarseGrainedExecutorBackend&lt;/a&gt;
is the main entry point class of executor process
which is able to be observed by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps -ef&lt;/code&gt; command.
As you dig deeper in the source code,
you should be able to find that each task is a thread submitted
to a thread pool.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark.executor.cores&lt;/code&gt; thus controls the upper limit of concurrent tasks
running in one executor process.
Their resources are shared within one process.
As quite a number of online sources have pointed out,
a big number of cores, say 8,
causes the overhead of context switch to be big and
actually slow down the overall performance.
A number between 2 and 4 (inclusive) is recommended for most Spark applications.&lt;/p&gt;

&lt;h3 id=&quot;yarn-mode&quot;&gt;YARN mode&lt;/h3&gt;

&lt;p&gt;YARN provides a much richer set of features such as
(very fine-grained) capacity scheduling, label-based scheduling and access control.&lt;/p&gt;

&lt;p&gt;The table below helps you compare and understand standalone and YARN mode
side by side.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;YARN&lt;/th&gt;
      &lt;th&gt;standalone&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ResourceManager&lt;/td&gt;
      &lt;td&gt;Master&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NodeManager&lt;/td&gt;
      &lt;td&gt;Worker&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yarn.nodemanager.resource.memory-mb&lt;/td&gt;
      &lt;td&gt;–memory&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yarn.nodemanager.resource.cpu-vcores&lt;/td&gt;
      &lt;td&gt;–cores&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A major difference between YARN and standalone mode in terms of resource control
is that workers stop spawning new executors
when either of the resources is exhausted.
However the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DefaultResourceCalculator&lt;/code&gt; only uses &lt;em&gt;memory&lt;/em&gt; to control
the resources used by executors.
As a result sometimes you can see that the available of vcores on a node
become negative.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DominantResourceCalculator&lt;/code&gt; behaves the same way as workers in standalone mode.
It chooses the dominant resource as the upper limit for resource usage.&lt;/p&gt;

&lt;h3 id=&quot;an-infra-perspective&quot;&gt;An infra perspective&lt;/h3&gt;

&lt;p&gt;I have heard that some companies use Kubernetes to spawn containers to host NodeManagers,
when there are huge demands for resources.&lt;/p&gt;

&lt;p&gt;An ideal scenario is where the memory and cores in your cluster
are consumed at the same pace.
When memory is used up,
cores should be exhausted as well.
To achieve better utility of your Hadoop slaves,
tune these parameters such that
when memory and cores are used up by executors,
memory in OS is close to fully utilized and CPU load is slightly below
maximum capacity.
This advice is given in condition that Spark executors are the dominant
processes running on your servers
and you should always leave some memory and computational power for
other processes than Spark executors and tasks.
Your Spark applications might have very strange behavior,
sometimes even failures,
when the CPU load of Hadoop slaves is extremely high.&lt;/p&gt;

&lt;p&gt;If you have both IO intensive (such as ETL)
and computationally intensive applications (such as data science apps),
consider introducing label-based scheduling before tuning.&lt;/p&gt;

&lt;h3 id=&quot;epilogue-data-engineering-in-shopee&quot;&gt;Epilogue: data engineering in Shopee&lt;/h3&gt;

&lt;p&gt;As the primary data provider in a leading E-commerce platform operating across Southeast Asia,
data engineering team is able to handle TB-level in one ETL flow and
this number is still increasing fast.
We run Spark jobs on top of an in-house Hadoop cluster
whose size is among the top-tier in Singapore as well as SEA region, hopefully.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="spark" /><summary type="html">There are tons of articles talking about setting the number of executors and cores for Spark applications. Hence I am not going to discuss anything about tuning these 2 parameters. Instead I would like to address what does the values of these parameters mean for nodes or containers which are executing your tasks. Summary The number of cores has no direct relation to physical CPU cores, instead it is a logical counter which determines the number of concurrent tasks that are able to run on one executor. I believe that many articles you can find have already addressed it. However from the perspective of servers or containers that receive tasks from the driver process, the number of cores is able to limit the total number of tasks running on one server/node/container. Standalone mode Standalone mode is very easy to setup but has few features in scheduling and access control. Though I have touched both standalone and YARN mode, starting from standalone mode is a good choice because the implementation details are easy to inspect, compared to YARN mode where lots of details are handled by YARN. Let’s start from worker process first. It is recommended that each server only runs one single worker process and I am going to treat it as an assumption. What are workers and executors? Each worker is Java process and so is an executor. If the number of cores is not specified when starting a worker, the worker will pick the number of physical cores from OS to be its capacity of cores. For Spark applications that demand executors, worker processes spawn executor processes as responses to such requests. Upon each creation of an executor process, worker deduct the amount of memory and cores from the total value it controls. In Worker class, you can find in method launchExecutor that for each launch of a new executor, the count of used cores is incremented. Therefore as I said, the number of memory and cores are more like logical values rather than the actual amount of resources each executor uses. What does it look like in deeper details? The number of memory (controlled by spark.executor.memory) determines the maximum heap size of executor process, by forming the -Xmx parameter. For example if you pass in --conf spark.executor.memory=10240m when submitting an Spark application, the executor Java process will include a command line argument as -Xmx 10240m. ExecutorRunner is the wrapper class around each executor process. It has an attribute which is an instance of Java Process. Actually CoarseGrainedExecutorBackend is the main entry point class of executor process which is able to be observed by using ps -ef command. As you dig deeper in the source code, you should be able to find that each task is a thread submitted to a thread pool. spark.executor.cores thus controls the upper limit of concurrent tasks running in one executor process. Their resources are shared within one process. As quite a number of online sources have pointed out, a big number of cores, say 8, causes the overhead of context switch to be big and actually slow down the overall performance. A number between 2 and 4 (inclusive) is recommended for most Spark applications. YARN mode YARN provides a much richer set of features such as (very fine-grained) capacity scheduling, label-based scheduling and access control. The table below helps you compare and understand standalone and YARN mode side by side. YARN standalone ResourceManager Master NodeManager Worker yarn.nodemanager.resource.memory-mb –memory yarn.nodemanager.resource.cpu-vcores –cores A major difference between YARN and standalone mode in terms of resource control is that workers stop spawning new executors when either of the resources is exhausted. However the DefaultResourceCalculator only uses memory to control the resources used by executors. As a result sometimes you can see that the available of vcores on a node become negative. DominantResourceCalculator behaves the same way as workers in standalone mode. It chooses the dominant resource as the upper limit for resource usage. An infra perspective I have heard that some companies use Kubernetes to spawn containers to host NodeManagers, when there are huge demands for resources. An ideal scenario is where the memory and cores in your cluster are consumed at the same pace. When memory is used up, cores should be exhausted as well. To achieve better utility of your Hadoop slaves, tune these parameters such that when memory and cores are used up by executors, memory in OS is close to fully utilized and CPU load is slightly below maximum capacity. This advice is given in condition that Spark executors are the dominant processes running on your servers and you should always leave some memory and computational power for other processes than Spark executors and tasks. Your Spark applications might have very strange behavior, sometimes even failures, when the CPU load of Hadoop slaves is extremely high. If you have both IO intensive (such as ETL) and computationally intensive applications (such as data science apps), consider introducing label-based scheduling before tuning. Epilogue: data engineering in Shopee As the primary data provider in a leading E-commerce platform operating across Southeast Asia, data engineering team is able to handle TB-level in one ETL flow and this number is still increasing fast. We run Spark jobs on top of an in-house Hadoop cluster whose size is among the top-tier in Singapore as well as SEA region, hopefully.</summary></entry><entry><title type="html">Offset topic and consumer group coordinator of Kafka</title><link href="https://wanwenli.com/kafka/2016/11/04/Kafka-Group-Coordinator.html" rel="alternate" type="text/html" title="Offset topic and consumer group coordinator of Kafka" /><published>2016-11-04T00:00:00+08:00</published><updated>2016-11-04T00:00:00+08:00</updated><id>https://wanwenli.com/kafka/2016/11/04/Kafka-Group-Coordinator</id><content type="html" xml:base="https://wanwenli.com/kafka/2016/11/04/Kafka-Group-Coordinator.html">&lt;p&gt;This article is to discuss two subjects
that are not frequently or clearly covered by official document or
online sources.&lt;/p&gt;

&lt;h4 id=&quot;offset-topic-the-__consumer_offsets-topic&quot;&gt;Offset topic (the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt; topic)&lt;/h4&gt;

&lt;p&gt;It is the only mysterious topic in Kafka log
and it cannot be deleted by using
&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/TopicCommand.scala&quot;&gt;TopicCommand&lt;/a&gt;.
Unfortunately there is no dedicated official documentation
to explain this internal topic.
The closest source I can find is
&lt;a href=&quot;http://www.slideshare.net/jjkoshy/offset-management-in-kafka&quot;&gt;this set of slides&lt;/a&gt;.
I strongly recommend reading it if you wish to understand
how this internal topic works.&lt;/p&gt;

&lt;p&gt;In short,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt; is used to store &lt;em&gt;offsets&lt;/em&gt; of consumers
which was previously stored only in ZooKeeper before version 0.8.1.1.
At the latest version of 0.8.X serious, i.e. 0.8.2.2,
the storage location of offsets can be configured by
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offsets.storage&lt;/code&gt; whose value can be either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zookeeper&lt;/code&gt;.
If it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka&lt;/code&gt;,
consumers are still able to commit offsets to ZooKeeper
by enabling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dual.commit.enabled&lt;/code&gt;.
However since version 0.9,
consumer offsets have been designed to be stored on brokers only.&lt;/p&gt;

&lt;p&gt;The partition key of the messages in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt;
was handled in
&lt;a href=&quot;https://github.com/apache/kafka/blob/0.8.2/core/src/main/scala/kafka/server/OffsetManager.scala&quot;&gt;OffsetManager&lt;/a&gt;
at version 0.8.X
and has been named as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OffsetKey&lt;/code&gt; in
&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala&quot;&gt;GroupMetadataManager&lt;/a&gt;
since version 0.9.0.
It contains three pieces of information: groupId, topic and partition number,
and the key is serialized/de-serialized according to a schema called
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OFFSET_COMMIT_KEY_SCHEMA&lt;/code&gt;.
The usage of schema is primarily for backward compatibility.
Unlike the behavior of
&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java&quot;&gt;DefaultPartitioner&lt;/a&gt;,
the partition number inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt;
determined by the hash value of partition key
but only determined by the hash value of consumer group,
which is as simple as&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Utils.abs(groupId.hashCode) % numPartitions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numPartitions&lt;/code&gt; is configured by the value of
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offsets.topic.num.partitions&lt;/code&gt; in broker configs
and is 50 by default.
This algorithm will be re-introduced later
in the part of consumer group coordination.&lt;/p&gt;

&lt;p&gt;The values of offset messages which is named
&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/common/OffsetMetadataAndError.scala&quot;&gt;OffsetMetadata&lt;/a&gt;
have two versions.
At version 0.8.X, the value contains offset, metadata (often empty)
and a timestamp
while the timestamp had been split into commit and expire timestamps
since version 0.9.0.
Console consumers are able to consume messages from internal topics
and print them out nicely.
A sample command has been shown below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./kafka-simple-consumer-shell.sh --topic __consumer_offsets \
--partition 49 \
--broker-list localhost:9092 \
--formatter &quot;kafka.server.OffsetManager\$OffsetsMessageFormatter&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By the way, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;absolute(&quot;testGroup&quot;.hashCode() % 50) = 49&lt;/code&gt;
which is the reason why partition 49 was specified.
It prints out:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[testGroup,testTopic-development,0]::OffsetAndMetadata[11,NO_METADATA,1478243992053]
[testGroup,testTopic-development,0]::OffsetAndMetadata[12,NO_METADATA,1478243992086]
[testGroup,testTopic-development,0]::OffsetAndMetadata[13,NO_METADATA,1478243992096]
[testGroup,testTopic-development,0]::OffsetAndMetadata[14,NO_METADATA,1478243992110]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However this is not the end of story,
because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt; is also used by group coordinator
to store group metadata!
The following section discusses another new feature
introduced since version 0.9.0.&lt;/p&gt;

&lt;h4 id=&quot;group-coordinator-coordinated-rebalance&quot;&gt;Group coordinator (coordinated rebalance)&lt;/h4&gt;

&lt;p&gt;This section is my humble and shallow understanding about
broker coordinator of consumer groups.
Correct me if I ever miss something or make any mistake.&lt;/p&gt;

&lt;p&gt;The introduction of coordinator, according to the
&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design&quot;&gt;official wiki of Kafka&lt;/a&gt;,
is to solve the &lt;em&gt;split brain&lt;/em&gt; problem
which is a well known problem in a distributed system.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="kafka" /><category term="consumer-group" /><summary type="html">This article is to discuss two subjects that are not frequently or clearly covered by official document or online sources. Offset topic (the __consumer_offsets topic) It is the only mysterious topic in Kafka log and it cannot be deleted by using TopicCommand. Unfortunately there is no dedicated official documentation to explain this internal topic. The closest source I can find is this set of slides. I strongly recommend reading it if you wish to understand how this internal topic works. In short, __consumer_offsets is used to store offsets of consumers which was previously stored only in ZooKeeper before version 0.8.1.1. At the latest version of 0.8.X serious, i.e. 0.8.2.2, the storage location of offsets can be configured by offsets.storage whose value can be either kafka or zookeeper. If it is kafka, consumers are still able to commit offsets to ZooKeeper by enabling dual.commit.enabled. However since version 0.9, consumer offsets have been designed to be stored on brokers only. The partition key of the messages in __consumer_offsets was handled in OffsetManager at version 0.8.X and has been named as OffsetKey in GroupMetadataManager since version 0.9.0. It contains three pieces of information: groupId, topic and partition number, and the key is serialized/de-serialized according to a schema called OFFSET_COMMIT_KEY_SCHEMA. The usage of schema is primarily for backward compatibility. Unlike the behavior of DefaultPartitioner, the partition number inside __consumer_offsets is not determined by the hash value of partition key but only determined by the hash value of consumer group, which is as simple as Utils.abs(groupId.hashCode) % numPartitions Here the numPartitions is configured by the value of offsets.topic.num.partitions in broker configs and is 50 by default. This algorithm will be re-introduced later in the part of consumer group coordination. The values of offset messages which is named OffsetMetadata have two versions. At version 0.8.X, the value contains offset, metadata (often empty) and a timestamp while the timestamp had been split into commit and expire timestamps since version 0.9.0. Console consumers are able to consume messages from internal topics and print them out nicely. A sample command has been shown below: ./kafka-simple-consumer-shell.sh --topic __consumer_offsets \ --partition 49 \ --broker-list localhost:9092 \ --formatter &quot;kafka.server.OffsetManager\$OffsetsMessageFormatter&quot; By the way, absolute(&quot;testGroup&quot;.hashCode() % 50) = 49 which is the reason why partition 49 was specified. It prints out: [testGroup,testTopic-development,0]::OffsetAndMetadata[11,NO_METADATA,1478243992053] [testGroup,testTopic-development,0]::OffsetAndMetadata[12,NO_METADATA,1478243992086] [testGroup,testTopic-development,0]::OffsetAndMetadata[13,NO_METADATA,1478243992096] [testGroup,testTopic-development,0]::OffsetAndMetadata[14,NO_METADATA,1478243992110] However this is not the end of story, because __consumer_offsets is also used by group coordinator to store group metadata! The following section discusses another new feature introduced since version 0.9.0. Group coordinator (coordinated rebalance) This section is my humble and shallow understanding about broker coordinator of consumer groups. Correct me if I ever miss something or make any mistake. The introduction of coordinator, according to the official wiki of Kafka, is to solve the split brain problem which is a well known problem in a distributed system.</summary></entry><entry><title type="html">Thoughts on Consumer Group of Apache Kafka</title><link href="https://wanwenli.com/kafka/2016/10/25/Kafka-Consumer-Group.html" rel="alternate" type="text/html" title="Thoughts on Consumer Group of Apache Kafka" /><published>2016-10-25T00:00:00+08:00</published><updated>2016-10-25T00:00:00+08:00</updated><id>https://wanwenli.com/kafka/2016/10/25/Kafka-Consumer-Group</id><content type="html" xml:base="https://wanwenli.com/kafka/2016/10/25/Kafka-Consumer-Group.html">&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;The concept of consumer group is intriguring.
According to the
&lt;a href=&quot;http://kafka.apache.org/documentation.html#introduction&quot;&gt;official documentation of Kafka&lt;/a&gt;,
one of its main objectives is to achieve message multicast and broadcast.
Often it is not an easy concept to grasp at the very beginning.
This article will share a few issues and discoveries about consumer group.&lt;/p&gt;

&lt;p&gt;A good way to start understanding consumer group is
looking at its data structure within ZooKeeper,
which is illustrated below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/consumer-group-zk.png&quot; alt=&quot;Your browser does not support img&quot; class=&quot;responsive-img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You may see each consumer group as a separated world in terms of message consumption.
Each consuemr group maintains its own consumer ID’s,
partition owners and offsets.&lt;/p&gt;

&lt;h4 id=&quot;rebalance&quot;&gt;Rebalance&lt;/h4&gt;

&lt;p&gt;Consumer rebalance happens when consumers join or leave a consumer group or
when the topics within a consumer group have new partitions.&lt;/p&gt;

&lt;p&gt;Consumer ID uniquely identitfies a consumer.
It is generated automatically when a consumer is launched.
The source code of its generation is defined in &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala&quot;&gt;ZookeeperConsumerConnector&lt;/a&gt;
and shown below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumerUuid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;consumerId&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;consumerId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// for testing only&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumerUuid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumerId&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// generate unique consumerId automatically&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;uuid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;UUID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;randomUUID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;consumerUuid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;%s-%d-%s&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;InetAddress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getLocalHost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getHostName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;currentTimeMillis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;uuid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getMostSignificantBits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toHexString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;substring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;groupId&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;_&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumerUuid&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you go into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/owners&lt;/code&gt; directory and check the owner of a partition,
you should find a very similar value to consumer ID.
The owner of a topic-partition is defined in the form of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[consumerId]-[threadId]&lt;/code&gt; in which &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;threadId&lt;/code&gt; is used
because one partition is meant to be consumed by exactly one thread.
In many cases, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;threadId&lt;/code&gt; is equal to the partition number which
the consumer thread owns.&lt;/p&gt;

&lt;p&gt;A consumer group is &lt;em&gt;well balanced&lt;/em&gt;
if each partition inside it is owned by exactly one consumer thread.
You may use the instructions from &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/System+Tools#SystemTools-VerifyConsumerRebalance&quot;&gt;this link&lt;/a&gt;
to verify the result of consumer rebalance.
This tool should work well because I have fixed a bug in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VerifyConsumerRebalance.scala&lt;/code&gt; in &lt;a href=&quot;https://github.com/apache/kafka/pull/1612&quot;&gt;this pull request&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since version 0.9.0,
Kafka has used &lt;em&gt;brokers&lt;/em&gt; to coordinate the rebalance process of consumers.
I also have thought about it and then wrote this blog.&lt;/p&gt;

&lt;h4 id=&quot;broadcast&quot;&gt;Broadcast&lt;/h4&gt;

&lt;p&gt;Broadcast by Kafka is relatively cheap:
you just need to put each consumer in different consumer groups,
such that the offset of each consumer is different.&lt;/p&gt;

&lt;p&gt;Once I read some articles online which suggest use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UUID&lt;/code&gt; within
a consumer group, for example &lt;a href=&quot;http://stackoverflow.com/questions/30647544/kafka-multiple-consumers-for-a-partition&quot;&gt;this question&lt;/a&gt; from stackoverflow and
it has received more than 5 upvotes.
Unfortunately I &lt;strong&gt;strongly discourage&lt;/strong&gt; such usage.&lt;/p&gt;

&lt;p&gt;Firstly and most importantly, consumer groups are stored as persistent nodes in ZooKeeper.
Often consumers need to be shutdown and started,
which may not be frequent in commercial environments
but is expected to be quite frequent in develop, test and staging phases.
As time goes by a huge number of consumer groups nodes
shall be accumulated in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/consumers&lt;/code&gt; node and
most of them are not in use.
A direct result is
it is almost impossible to list out all the consumer groups.
ZooKeeper client throws runtime exception because the buffer overflows due to
the overwhelming amount of child nodes.
The error message is something like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;IOException Packet &amp;lt;len12343123123&amp;gt; is out of range
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even you configure the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jute.maxbuffer&lt;/code&gt; parameter as some blogs suggest,
the chance of receiving a response
before your patience runs out is extremely low.
I have encountered a situation where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/consumers&lt;/code&gt; has more than 90,000
child nodes and was unable to list up consumer groups ever since.
Another direct result is that some features of &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala&quot;&gt;ConsumerGroupCommand&lt;/a&gt;
(used in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-consumer-groups.sh&lt;/code&gt;)
will fail because internally it lists out all consumer groups.&lt;/p&gt;

&lt;p&gt;However, the huge number of consumer groups should &lt;em&gt;not&lt;/em&gt; be a problem
for the normal operation of Kafka,
because the brokers do not need to know all the groups.
Even though from version 0.9 onward,
consumer rebalance is coordinated by brokers,
brokers do not scan groups when deciding which broker coordinates
which subset of them.
They only manage those consumer which join or leave them.
As far as what I have read from &lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/GroupCoordinator.scala&quot;&gt;GroupCoordinator&lt;/a&gt;,
brokers do not scan all consumer groups.&lt;/p&gt;

&lt;p&gt;Next, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UUID&lt;/code&gt; is hardly queryable.
Its value is generated at runtime and unpredictable.
If you wish to describe a consumer group or check its offsets,
you will face problems on finding the exact value of it.
A workaround is to &lt;em&gt;log&lt;/em&gt; its value somewhere.&lt;/p&gt;

&lt;p&gt;Last but not the least,
group names with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UUID&lt;/code&gt; in them is brand new every time.
Upon the creation of a new consumer group,
the value of offsets is determined by offset reset,
in other words the setting of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.offset.reset&lt;/code&gt;.
No matter the value is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;smallest&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;largest&lt;/code&gt;,
the consumer either consumes many messages repeatedly or
skip some messages.&lt;/p&gt;

&lt;h4 id=&quot;delete-a-group&quot;&gt;Delete a group&lt;/h4&gt;

&lt;p&gt;Deleting a consumer group from ZooKeeper is
as easy as one single instruction.
However, it implies that the offset information
will be permantly deleted from ZooKeeper.
Do it only when you are 100% sure that
the target group will never be reused.&lt;/p&gt;

&lt;p&gt;From version 0.9.0 onward,
&lt;a href=&quot;https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala&quot;&gt;ConsumerGroupCommand&lt;/a&gt;
introduces a feature to remove consumer groups.
Nevertheless, it prohibits the removal of &lt;em&gt;active&lt;/em&gt; consumer group by
checking the number of children under consuemr registry dicrectory
(i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/consumers/[group_name]/ids&lt;/code&gt;).
When you wish to remove a consumer group by a ZooKeeper client,
please take notice of active consumers as well.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="kafka" /><category term="consumer-group" /><summary type="html">Introduction The concept of consumer group is intriguring. According to the official documentation of Kafka, one of its main objectives is to achieve message multicast and broadcast. Often it is not an easy concept to grasp at the very beginning. This article will share a few issues and discoveries about consumer group. A good way to start understanding consumer group is looking at its data structure within ZooKeeper, which is illustrated below. You may see each consumer group as a separated world in terms of message consumption. Each consuemr group maintains its own consumer ID’s, partition owners and offsets. Rebalance Consumer rebalance happens when consumers join or leave a consumer group or when the topics within a consumer group have new partitions. Consumer ID uniquely identitfies a consumer. It is generated automatically when a consumer is launched. The source code of its generation is defined in ZookeeperConsumerConnector and shown below. var consumerUuid : String = null config.consumerId match { case Some(consumerId) // for testing only =&amp;gt; consumerUuid = consumerId case None // generate unique consumerId automatically =&amp;gt; val uuid = UUID.randomUUID() consumerUuid = &quot;%s-%d-%s&quot;.format( InetAddress.getLocalHost.getHostName, System.currentTimeMillis, uuid.getMostSignificantBits().toHexString.substring(0,8)) } config.groupId + &quot;_&quot; + consumerUuid If you go into /owners directory and check the owner of a partition, you should find a very similar value to consumer ID. The owner of a topic-partition is defined in the form of [consumerId]-[threadId] in which threadId is used because one partition is meant to be consumed by exactly one thread. In many cases, the threadId is equal to the partition number which the consumer thread owns. A consumer group is well balanced if each partition inside it is owned by exactly one consumer thread. You may use the instructions from this link to verify the result of consumer rebalance. This tool should work well because I have fixed a bug in VerifyConsumerRebalance.scala in this pull request. Since version 0.9.0, Kafka has used brokers to coordinate the rebalance process of consumers. I also have thought about it and then wrote this blog. Broadcast Broadcast by Kafka is relatively cheap: you just need to put each consumer in different consumer groups, such that the offset of each consumer is different. Once I read some articles online which suggest use UUID within a consumer group, for example this question from stackoverflow and it has received more than 5 upvotes. Unfortunately I strongly discourage such usage. Firstly and most importantly, consumer groups are stored as persistent nodes in ZooKeeper. Often consumers need to be shutdown and started, which may not be frequent in commercial environments but is expected to be quite frequent in develop, test and staging phases. As time goes by a huge number of consumer groups nodes shall be accumulated in /consumers node and most of them are not in use. A direct result is it is almost impossible to list out all the consumer groups. ZooKeeper client throws runtime exception because the buffer overflows due to the overwhelming amount of child nodes. The error message is something like: IOException Packet &amp;lt;len12343123123&amp;gt; is out of range Even you configure the jute.maxbuffer parameter as some blogs suggest, the chance of receiving a response before your patience runs out is extremely low. I have encountered a situation where /consumers has more than 90,000 child nodes and was unable to list up consumer groups ever since. Another direct result is that some features of ConsumerGroupCommand (used in kafka-consumer-groups.sh) will fail because internally it lists out all consumer groups. However, the huge number of consumer groups should not be a problem for the normal operation of Kafka, because the brokers do not need to know all the groups. Even though from version 0.9 onward, consumer rebalance is coordinated by brokers, brokers do not scan groups when deciding which broker coordinates which subset of them. They only manage those consumer which join or leave them. As far as what I have read from GroupCoordinator, brokers do not scan all consumer groups. Next, UUID is hardly queryable. Its value is generated at runtime and unpredictable. If you wish to describe a consumer group or check its offsets, you will face problems on finding the exact value of it. A workaround is to log its value somewhere. Last but not the least, group names with UUID in them is brand new every time. Upon the creation of a new consumer group, the value of offsets is determined by offset reset, in other words the setting of auto.offset.reset. No matter the value is smallest or largest, the consumer either consumes many messages repeatedly or skip some messages. Delete a group Deleting a consumer group from ZooKeeper is as easy as one single instruction. However, it implies that the offset information will be permantly deleted from ZooKeeper. Do it only when you are 100% sure that the target group will never be reused. From version 0.9.0 onward, ConsumerGroupCommand introduces a feature to remove consumer groups. Nevertheless, it prohibits the removal of active consumer group by checking the number of children under consuemr registry dicrectory (i.e., /consumers/[group_name]/ids). When you wish to remove a consumer group by a ZooKeeper client, please take notice of active consumers as well.</summary></entry><entry><title type="html">Advice for STM R&amp;amp;D Assignments</title><link href="https://wanwenli.com/programming/2014/12/28/STM-RD-Advice.html" rel="alternate" type="text/html" title="Advice for STM R&amp;D Assignments" /><published>2014-12-28T00:00:00+08:00</published><updated>2014-12-28T00:00:00+08:00</updated><id>https://wanwenli.com/programming/2014/12/28/STM-RD-Advice</id><content type="html" xml:base="https://wanwenli.com/programming/2014/12/28/STM-RD-Advice.html">&lt;h5 id=&quot;introduction--disclaimer&quot;&gt;Introduction &amp;amp; Disclaimer&lt;/h5&gt;

&lt;p&gt;STM (Starter Mission) is the training program for
new employees of
&lt;a href=&quot;http://www.worksap.com/&quot;&gt;Works Applications Co., Ltd.&lt;/a&gt;
Each employee is  required to implement
several information systems.
Here are some practical guidelines for you
to program and deliver your systems more effectively.&lt;/p&gt;

&lt;p&gt;Here we use a simple leave-application system as an illustration.
It has some basic requirements as below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are two roles in the system: manager and employees.&lt;/li&gt;
  &lt;li&gt;Managers should see applications of leaves from employees.&lt;/li&gt;
  &lt;li&gt;Managers is able to approve or reject an application.&lt;/li&gt;
  &lt;li&gt;The system may have other features such as departments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the main feature is the application of leaves.&lt;/p&gt;

&lt;p&gt;Disclaimer:
this article does not provide solutions to any assignment in STM.
It never guarantees your success in STM.
The author is not responsible for any failure
because of taking the advice in this article.&lt;/p&gt;

&lt;h5 id=&quot;maintain-a-good-quality-of-source-code&quot;&gt;Maintain a good quality of source code&lt;/h5&gt;
&lt;p&gt;Perhaps you have not programmed for months before STM and
you want to pass STM as soon as possible.
However these are not excuses to sacrifice your
code quality for faster development.
Believe me that you will probably spend more time
due to poor code quality.&lt;/p&gt;

&lt;h5 id=&quot;think-about-the-five-principles&quot;&gt;Think about the five principles&lt;/h5&gt;

&lt;p&gt;You may have heard of the SOLID principles,
which are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Single_responsibility_principle&quot;&gt;Single responsibility&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Open/closed_principle&quot;&gt;Open/closed&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Liskov_substitution_principle&quot;&gt;Liskov substitution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Interface_segregation_principle&quot;&gt;Interface segregation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Dependency_inversion_principle&quot;&gt;Dependency inversion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two principles helped me a lot
during the design of my own system.
Ask yourself when writing a class or method:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Does it perform a single task for one responsibility?&lt;/li&gt;
  &lt;li&gt;Is it closed for modification and open for extension?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example,
combining adding and updating methods into one method called
&lt;em&gt;apply()&lt;/em&gt; is not a good idea.
They had better be separated.
Another example is to implement public method cautiously.&lt;/p&gt;

&lt;p&gt;Remember: if you strictly follow single responsibility principle,
open/closed principle would be natural for you as well.&lt;/p&gt;

&lt;h5 id=&quot;controller---the-key-part-between-view-and-model&quot;&gt;Controller - the key part between view and model&lt;/h5&gt;

&lt;p&gt;Each controller should be responsible for only one feature.
It should not be defined by roles in the system.
One controller should be responsible for CRUD
(create, read, update and delete)
of exactly one feature, e.g., &lt;em&gt;UserController&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In addition,
input validation and page redirection methods
should be separated from controllers as well.
Some people believe that the input validation logic should be bounded to each entity,
such that the data inside an entity is consistent wherever it is used.&lt;/p&gt;

&lt;h5 id=&quot;build-up-ui-from-components&quot;&gt;Build up UI from components&lt;/h5&gt;

&lt;p&gt;Break down one page into different components and reuse them.
It saves you lots of time and greatly improves the maintainability of your UI.
&lt;a href=&quot;http://www.tutorialspoint.com/jsf/jsf_facelets_tags.htm&quot;&gt;This&lt;/a&gt;
simple tutorial will help you decompose your UI.&lt;/p&gt;

&lt;h5 id=&quot;sql-builder-optional&quot;&gt;SQL builder (optional)&lt;/h5&gt;

&lt;p&gt;Write a SQL builder (wrapper) to generate SQL strings.
It does not save you lots of time,
but it improves maintainability of your SQL.&lt;/p&gt;

&lt;h5 id=&quot;dictionary-generator-optional&quot;&gt;Dictionary generator (optional)&lt;/h5&gt;

&lt;p&gt;If you need to implement a dictionary feature,
consider writing a program to automatically generate dictionary insertion SQL.&lt;/p&gt;

&lt;h5 id=&quot;what-is-a-merit&quot;&gt;What is a merit?&lt;/h5&gt;

&lt;p&gt;You need to be very clear on this principle since the beginning of STM:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Not every feature is a merit.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some features are necessary for daily business operation.
Merits are always related to benefits.
They are those features that generate revenue,
improve efficiency,
enhance security,
ensure the stability of the system and operations.&lt;/p&gt;

&lt;p&gt;Therefore when you write a catalog or implement a feature,
always ask yourself these questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What merits does it bring to the user?&lt;/li&gt;
  &lt;li&gt;What are the benefits of using my system compared to using just paper and pen?&lt;/li&gt;
  &lt;li&gt;What makes my client want to buy my system?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Take the leave-application system for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Does it really help the manager to maintain the leaves of his employees?&lt;/li&gt;
  &lt;li&gt;Does it ease the process of applying and maintaining leaves?&lt;/li&gt;
  &lt;li&gt;What is the advantage of my system over writing leaves on paper?&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;is-the-ui-intuitive-to-use&quot;&gt;Is the UI intuitive to use?&lt;/h5&gt;

&lt;p&gt;Since you have stared at your system for a very long time,
you should be very familiar with it.
But the customer who use your system may not be.
Ask yourself these questions on each page (view):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Is the user able to see the information presented in a clear and organized manner?&lt;/li&gt;
  &lt;li&gt;Does the user know what he is doing and what to do next from the UI?&lt;/li&gt;
  &lt;li&gt;Does the design and flow of my system effectively reduces clicks and page navigations?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If your answer is negative or not sure,
consider improving your UI.
The user should be able to use your system by
reading as few instructions as possible.&lt;/p&gt;

&lt;p&gt;For example, consider using a
&lt;a href=&quot;http://www.primefaces.org/showcase/ui/panel/wizard.xhtml&quot;&gt;wizard&lt;/a&gt;
if there is a flow.
Give warning or confirmation before irrecoverable operations,
such as deletion and payment.
Do not purposefully choose fanciful (and complicated) UI component
just because it looks nice.
Simplicity has its value.&lt;/p&gt;

&lt;h5 id=&quot;robust-testing-using-test-cases&quot;&gt;Robust testing using test cases&lt;/h5&gt;

&lt;p&gt;Your system is expected to be bug free
because nobody wants to buy a buggy product.&lt;/p&gt;

&lt;p&gt;Write down test cases to systematically test your system.
Do not just randomly click around on the screen and
wish bugs will pop out by themselves.&lt;/p&gt;

&lt;p&gt;If possible, automate the test, for example:
run unit test for Util classes.&lt;/p&gt;

&lt;p&gt;Learn to use debug mode instead of printing messages into console.&lt;/p&gt;

&lt;h5 id=&quot;clear-catalog-show-how-your-system-flows&quot;&gt;Clear catalog: show how your system flows&lt;/h5&gt;

&lt;p&gt;My personal suggestion for your catalog is to
write use cases to demonstrate at least one complete flow of your system.&lt;/p&gt;

&lt;p&gt;For example, your catalog should present the following work flow:
manager creates an employee profile -&amp;gt;
employee applies for a leave -&amp;gt;
manager approves that application -&amp;gt;
employee takes the leave -&amp;gt;
employee returns to work later than expected -&amp;gt;
manager updates the employee`s leave record&lt;/p&gt;

&lt;h5 id=&quot;summary&quot;&gt;Summary&lt;/h5&gt;

&lt;p&gt;STM is not meant to fail you.
It is designed to equip you with
essential knowledge and skills for future work.
Please fully use the time and resources during STM and
it will surely benefit you in the days to come.
I wish you all the best.&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="programming" /><category term="life" /><category term="career" /><summary type="html">Introduction &amp;amp; Disclaimer STM (Starter Mission) is the training program for new employees of Works Applications Co., Ltd. Each employee is required to implement several information systems. Here are some practical guidelines for you to program and deliver your systems more effectively. Here we use a simple leave-application system as an illustration. It has some basic requirements as below: There are two roles in the system: manager and employees. Managers should see applications of leaves from employees. Managers is able to approve or reject an application. The system may have other features such as departments. But the main feature is the application of leaves. Disclaimer: this article does not provide solutions to any assignment in STM. It never guarantees your success in STM. The author is not responsible for any failure because of taking the advice in this article. Maintain a good quality of source code Perhaps you have not programmed for months before STM and you want to pass STM as soon as possible. However these are not excuses to sacrifice your code quality for faster development. Believe me that you will probably spend more time due to poor code quality. Think about the five principles You may have heard of the SOLID principles, which are Single responsibility Open/closed Liskov substitution Interface segregation Dependency inversion The first two principles helped me a lot during the design of my own system. Ask yourself when writing a class or method: Does it perform a single task for one responsibility? Is it closed for modification and open for extension? For example, combining adding and updating methods into one method called apply() is not a good idea. They had better be separated. Another example is to implement public method cautiously. Remember: if you strictly follow single responsibility principle, open/closed principle would be natural for you as well. Controller - the key part between view and model Each controller should be responsible for only one feature. It should not be defined by roles in the system. One controller should be responsible for CRUD (create, read, update and delete) of exactly one feature, e.g., UserController. In addition, input validation and page redirection methods should be separated from controllers as well. Some people believe that the input validation logic should be bounded to each entity, such that the data inside an entity is consistent wherever it is used. Build up UI from components Break down one page into different components and reuse them. It saves you lots of time and greatly improves the maintainability of your UI. This simple tutorial will help you decompose your UI. SQL builder (optional) Write a SQL builder (wrapper) to generate SQL strings. It does not save you lots of time, but it improves maintainability of your SQL. Dictionary generator (optional) If you need to implement a dictionary feature, consider writing a program to automatically generate dictionary insertion SQL. What is a merit? You need to be very clear on this principle since the beginning of STM: Not every feature is a merit. Some features are necessary for daily business operation. Merits are always related to benefits. They are those features that generate revenue, improve efficiency, enhance security, ensure the stability of the system and operations. Therefore when you write a catalog or implement a feature, always ask yourself these questions: What merits does it bring to the user? What are the benefits of using my system compared to using just paper and pen? What makes my client want to buy my system? Take the leave-application system for example: Does it really help the manager to maintain the leaves of his employees? Does it ease the process of applying and maintaining leaves? What is the advantage of my system over writing leaves on paper? Is the UI intuitive to use? Since you have stared at your system for a very long time, you should be very familiar with it. But the customer who use your system may not be. Ask yourself these questions on each page (view): Is the user able to see the information presented in a clear and organized manner? Does the user know what he is doing and what to do next from the UI? Does the design and flow of my system effectively reduces clicks and page navigations? If your answer is negative or not sure, consider improving your UI. The user should be able to use your system by reading as few instructions as possible. For example, consider using a wizard if there is a flow. Give warning or confirmation before irrecoverable operations, such as deletion and payment. Do not purposefully choose fanciful (and complicated) UI component just because it looks nice. Simplicity has its value. Robust testing using test cases Your system is expected to be bug free because nobody wants to buy a buggy product. Write down test cases to systematically test your system. Do not just randomly click around on the screen and wish bugs will pop out by themselves. If possible, automate the test, for example: run unit test for Util classes. Learn to use debug mode instead of printing messages into console. Clear catalog: show how your system flows My personal suggestion for your catalog is to write use cases to demonstrate at least one complete flow of your system. For example, your catalog should present the following work flow: manager creates an employee profile -&amp;gt; employee applies for a leave -&amp;gt; manager approves that application -&amp;gt; employee takes the leave -&amp;gt; employee returns to work later than expected -&amp;gt; manager updates the employee`s leave record Summary STM is not meant to fail you. It is designed to equip you with essential knowledge and skills for future work. Please fully use the time and resources during STM and it will surely benefit you in the days to come. I wish you all the best.</summary></entry><entry><title type="html">It has been two years</title><link href="https://wanwenli.com/life/2014/02/03/TwoYears.html" rel="alternate" type="text/html" title="It has been two years" /><published>2014-02-03T00:00:00+08:00</published><updated>2014-02-03T00:00:00+08:00</updated><id>https://wanwenli.com/life/2014/02/03/TwoYears</id><content type="html" xml:base="https://wanwenli.com/life/2014/02/03/TwoYears.html">&lt;p&gt;It has been two years since I joined &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/github-reg.png&quot; alt=&quot;Your browser does not support img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since then my life has stepped into AD (After Development) from
BC (Before Coding).&lt;/p&gt;</content><author><name>Wan Wenli</name></author><category term="life" /><category term="github" /><summary type="html">It has been two years since I joined GitHub. Since then my life has stepped into AD (After Development) from BC (Before Coding).</summary></entry></feed>