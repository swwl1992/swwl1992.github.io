<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title>How do Bitcoin mining pools pay their miners?</title>
      <url>/blockchain/2018/09/13/How-do-bitcoin-mining-pools-pay.html</url>
      <content type="text">Since the price of Bitcoin surged in the 2017 Q4,
many mining pools have surfaced.
Over the globe there are over 20 active and recognizable mining pools.
You can find more pools and their addresses from
this GitHub repo.
This article explains a particular type of routine job in a Bitcoin mining pool:
how does it pay Bitcoins to its miners?
Actually you will find out that it might not be the same as you thought.

Directly from coinbase addresses

This is the simplest way.
SlushPool has been using this method for some time.
You can easily find payout transactions miners from address
1CK6KHY6MHgYvmRQ4PAafKYDrg1ejbH1cE
which is the publicly known coinbase address of SlushPool.
On a typical day you can observe more than five transactions from this address
each of which is with a huge number of outputs to various addresses -
it is the typical way of identifying payments from a mining pool to its miners.

Other pools that also use this method are
BTC.TOP and Huobi Pool.
However, you can find that BTC.TOP also uses intermediate addresses to pay its miners.

Pay via intermediate wallets

This is probably the most common practice of pools.
Take BTC.com for example,
you can find its coinbase address is
bc1qjl8uwezzlech723lpnyuza0h2cdkvxvh54v3dn.
There are many transactions associated with this address,
most of which are coinbase transactions.
However there are some Bitcoins sent by it to other addresses,
for example,
3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2
in transaction
d094eca893253c12bb53f55ed834281349eda9e59505fa91d07e114fd616aa02.

Therefore,
3FxUA8godrRmxgUaPv71b3XCUxcoCLtUx2
is one of the intermediate addresses used by BTC.com.
Every day there are a few transactions from it to many addresses,
probably thousands of outputs in a single transaction,
and this is how BTC.com pays its miners.

AntPool: payments chain

AntPool is a very special case and
its payment pattern is the most difficult to trace.
The whole process of payments is like a chain or loop.
Almost every day it starts from in the morning
and last until late afternoon.


  Generate a new wallet
  Use the intermediate address to pay 101 addresses
  Among the 101 recipients 100 are miners,
while the one newly generated wallet receives all the change
  Use the balance in the change wallet to pay another 100 miners + 1 new wallet
  Repeat the process above until all miner addresses are paid


The end result is that all the intermediate change wallets are empty and
they will not be reused.
Only the very last change wallet has some Bitcoins in it.
Personally I like this process because it gives the miners highest level of
privacy.
If you wish to find out all miners’ addresses under AntPool by hands,
you have to traverse many transactions.
But for other pools, you only need to checkout less than ten transactions,
probably even less than five.

F2Pool: pay without transaction fees

Everyday F2Pool broadcasts a transaction with zero transaction fee
to pay its miners.
After that, F2Pool tries to mine it because
no other miner is willing to mine a transaction without any fee.
This approach saves money, apparently.
A major disadvantage of it is there are huge
uncertainties in the time when its miners receive Bitcoins.
</content>
      <categories>
        
          <category> blockchain </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>A brief guide to Bitcoin lightning network with examples</title>
      <url>/blockchain/2018/06/28/Bitcoin-lightning-network.html</url>
      <content type="text">Why another post about the lightning network (LN) of Bitcoin?

The primary reason is that
some of the resources online are not up to date.
The technological details of LN are not intuitive enough
and not easy to grasp.
Without examples, confusion and misunderstanding may arise.

If you are a technician, researcher, engineer,
blockchain hobbyist or enthusiast
who already has some knowledge about LN,
this article is for you.
Do not worry if you know nothing about LN,
because this article leads you to the most useful resources
and tell you which piece of information is accurate in them.
Some understanding about
segregated witness
is strongly recommended before
reading the remaining part of this post,
as current LN payments heavily rely on it.

In summary this post focuses on:


  what are the reliable resources to read/watch in order to understand LN;
  what info inside these articles and videos is accurate and what is outdated;
  how LN works, roughly;
  how to find LN transactions on-chain.


Unfortunately, this article does not discuss
how lightning network will affect bitcoin ecosystem,
its advantages and drawbacks,
how much transaction rate will improve, or even coin price!

Good news: this article provides concrete examples of payments via LN
that have been permanently written on Bitcoin blockchain!
So far I have not seen any other articles that give out examples.

Last but not the least,
this post is my personal humble understanding on LN.
I am open for comments and critics.

Articles and videos

The GitHub repo
awesome-lightning-network
lists out many resources for LN.
It is a good starting point.
However in my point of view,
there is only one that is sufficiently comprehensive: the
lightning network paper.
I strongly recommend you to read through it.

You may have come across
this YouTube video
on LN.
It appears as the 2nd search result of
“lightning network” on YouTube.
Its weakness is that for bi-directional payments,
how to penalise an old commitment transaction and
revoke all the funds is not clear.
To understand the details of penalty
which is an essential part to ensure the safety and trustworthiness of LN,
please read the LN paper.
Penalty on HTLC transactions follows a very similar mechanism.
Later you will see both successfully claimed and penalised fund on-chain.

Besides, the articles and videos share another fatal issue:
the scripts illustrated in them cannot be found on the most recent blocks.
For example, I am unable to find an input script
that follow the format below which is on page 31 of the LN paper
and implemented in a JavaScript LN protocol,
yours-channels.
However if you do find one, please leave your comment.

OP IF
  OP HASH160 &amp;lt;Hash160 (R)&amp;gt; OP EQUALVERIFY
  2 &amp;lt;Ali c e 2&amp;gt; &amp;lt;Bob2&amp;gt; OP CHECKMULTISIG
OP ELSE
  2 &amp;lt;Ali c e 1&amp;gt; &amp;lt;Bob1&amp;gt; OP CHECKMULTISIG
OP ENDIF


I suppose it is because
the specifications of LN have been updated and redefined in
this repo
called Basis of Lightning Technology (BOLT).
To be more specific, transactions and scripts are documented in
BOLT #3.

Source code

There is a handful of implementations for LN.
The most popular two are the
LND written in Go
and
lightning in C.
I have found these two repos after encountering the
mainnet LN explorer
which gets readings from them.

Both of them strictly follow the
BOLT #3
to build transactions and scripts.
Read more detailed source code here:


  script_utils.go
in LND
  script.c in lightning


As you will see soon,
BOLT #3 is the dominantly (probably the only) observable form on-chain.

How to find a LN transaction

There are two major forms of transactions that are surely from LN:


  2-of-2-MULTISIG embedded in P2WSH followed by
a conditional time locked input (for bi-directional channels)
  2-of-2-MULTISIG embedded in P2WSH followed by at least one HTLC input


The first one is the dominant type of input script in LN and
takes up more than 80% of transaction inputs broadcast by LN.
Later you will see that the second form has two versions,
one for sender and the other for receiver.

Examples

Here are some example hashes of LN transactions.

to_local

It is used to close a bi-directional channel and is
the most commonly seen form, as mentioned before.


  succeeded:
0191535bfda21f5dfec1c904775c5e2fbee8a985815c88d77258a0b42dba3526
  penalised:
0da5e5dba5e793d50820c2275dab74912b121c8b7e34ce32a9dbfd4567a9bf8e


Let us examine the successful case:


  Its upstream input is a 2-of-2 MULTISIG embedded in P2WSH,
the funding transaction.
  The funding transaction has two outputs (commitment transactions)
and the other is a P2WPKH.
The P2WPKH is not time locked and
has been sent to the counterparty of the channel.
  Its input witness starts with &amp;lt;sig&amp;gt; 0 form
therefore its fund has already been successfully taken.
According to the LN paper, the LN channel is fully closed.


Why the second input has been penalised?
Because there is a 1 right after the signature and
it executes the if-branch which revokes all the funds in the channel.
Both inputs were from the two outputs of
the previous funding transaction and there is only one output.
Therefore we can conclude that it is in a penalty transaction.

sender script

Here are two examples:


  timeout:
a16f6d78a58d31fe7459887adf5bd6b4dd95277ea375d250c700cde9fa908bdb
  claimed by preimage:
89c744f0806a57a9b4634c320703cc941aaf272f290296373b709499064335e5


The expenditures by timeout are easy to identify, as its format is simply
0 &amp;lt;sig&amp;gt; &amp;lt;sig&amp;gt; 0.
How to identify if a HTLC transaction has used the preimage?
Here I would like to recommend the
python bitcoin library.
I like it a lot because python provides power interactive terminal
and it is convenient to analyse bitcoin blockchain.

Let us see the transaction input whose hash starts with “89c744”.
Its witness is in the form of sig [unknown] scriptPubKey.
Later we will find out that the unknown part is
actually the hash160 of a secret, the hash that locks the contract.
A revocation script will appear in the exactly same form
but we shall see that the script does not execute the revocation path.

The scriptPubKey part, according to BOLT #3, is decoded as

In [1]: from bitcoin.core import CScript

In [2]: CScript(bytearray.fromhex('76a9149d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd38763ac672102aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c865977c820120876475527c210362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a52ae67a9146e3bef3f86aed6d6f825f13d1fa070039c866c5c88ac6868'))
Out[2]: CScript([OP_DUP, OP_HASH160, x('9d908ebcc9e1913b808eb7b4ba47bc4d1b35ebd3'), OP_EQUAL, OP_IF, OP_CHECKSIG, OP_ELSE, x('02aa52226cbb5aaef23f175575c06feb16aa303e76f288be28c4760ef768c86597'), OP_SWAP, OP_SIZE, x('20'), OP_EQUAL, OP_NOTIF, OP_DROP, 2, OP_SWAP, x('0362c9ec1c0c7bb399037469b376e9e19d6aa0fbb58df811786b7425dea94b519a'), 2, OP_CHECKMULTISIG, OP_ELSE, OP_HASH160, x('6e3bef3f86aed6d6f825f13d1fa070039c866c5c'), OP_EQUALVERIFY, OP_CHECKSIG, OP_ENDIF, OP_ENDIF])


Next, let us see the hash value of the unknown part,

In [1]: import bitcoin

In [2]: CScript([bitcoin.core.Hash160(bytearray.fromhex('ae626cc4d6c208bdb3179b9d3efc7ae61779a9924b3852f01d0024afa84a4bbb'))])
Out[2]: CScript([x('6e3bef3f86aed6d6f825f13d1fa070039c866c5c')])


See? The hash160 of the previously unknown field is
equal to the value right after the last OP_HASH160
and before the OP_EQUALVERIFY.
Therefore the fund is indeed taken by providing the preimage,
according to the HTLC script for senders defined in BOLT #3,
instead of being revoked.

Here I am not going to show how the script should be executed
step by step on a stack.
You may do this yourself and keep in mind that
the exection path in a if-else branch is determined by a digit
whose value is either 1 or 0.

receiver script

Similarly, here are another two examples for receiver script:


  claimed by preimage:
36b1aff2ad0076be95b1ee1dc4036374998760c80c6583a6478a699e86658ac0
  timeout:
f9af9b93d66c7e5ee7dcbe0b53faa3d17aa6b9f4cc5b19f0985917b57d82c59a


Again, if you find HTLC inputs that are penalised,
be sure to leave a comment.

Summary

With all the resources presented by this article
and on-chain examples,
I hope you can better understand transactions in LN.

Long live blockchain.
</content>
      <categories>
        
          <category> blockchain </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Apache Spark's number of cores rethought</title>
      <url>/spark/2018/04/02/spark-executor-cores.html</url>
      <content type="text">There are tons of articles talking about setting the number of executors and cores for Spark applications.
Hence I am not going to discuss anything about tuning these 2 parameters.
Instead I would like to address what does the values of these parameters mean
for nodes or containers which are executing your tasks.

Summary

The number of cores has no direct relation to physical CPU cores,
instead it is a logical counter
which determines the number of concurrent
tasks that are able to run on one executor.
I believe that many articles you can find have already addressed it.
However from the perspective of servers or containers
that receive tasks from the driver process,
the number of cores is able to limit the total number of tasks running
on one server/node/container.

Standalone mode

Standalone mode is very easy to setup but has few features in
scheduling and access control.
Though I have touched both standalone and YARN mode,
starting from standalone mode is a good choice
because the implementation details are easy to inspect, compared to YARN mode
where lots of details are handled by YARN.

Let’s start from worker process first.
It is recommended that each server only runs one single worker process
and I am going to treat it as an assumption.

What are workers and executors?

Each worker is Java process and so is an executor.
If the number of cores is not specified when starting a worker,
the worker will pick the number of physical cores from OS
to be its capacity of cores.
For Spark applications that demand executors,
worker processes spawn executor processes as responses to such requests.
Upon each creation of an executor process,
worker deduct the amount of memory and cores from the total value
it controls.
In Worker
class, you can find in method launchExecutor that
for each launch of a new executor, the count of used cores is incremented.
Therefore as I said,
the number of memory and cores are more like
logical values rather than the actual amount of resources each executor uses.

What does it look like in deeper details?
The number of memory (controlled by spark.executor.memory)
determines the maximum heap size of executor process,
by forming the -Xmx parameter.
For example if you pass in --conf spark.executor.memory=10240m
when submitting an Spark application,
the executor Java process will include a command line argument as -Xmx 10240m.

ExecutorRunner
is the wrapper class around each executor process.
It has an attribute which is an instance of Java
Process.
Actually CoarseGrainedExecutorBackend
is the main entry point class of executor process
which is able to be observed by using ps -ef command.
As you dig deeper in the source code,
you should be able to find that each task is a thread submitted
to a thread pool.
spark.executor.cores thus controls the upper limit of concurrent tasks
running in one executor process.
Their resources are shared within one process.
As quite a number of online sources have pointed out,
a big number of cores, say 8,
causes the overhead of context switch to be big and
actually slow down the overall performance.
A number between 2 and 4 (inclusive) is recommended for most Spark applications.

YARN mode

YARN provides a much richer set of features such as
(very fine-grained) capacity scheduling, label-based scheduling and access control.

The table below helps you compare and understand standalone and YARN mode
side by side.


  
    
      YARN
      standalone
    
  
  
    
      ResourceManager
      Master
    
    
      NodeManager
      Worker
    
    
      yarn.nodemanager.resource.memory-mb
      –memory
    
    
      yarn.nodemanager.resource.cpu-vcores
      –cores
    
  


A major difference between YARN and standalone mode in terms of resource control
is that workers stop spawning new executors
when either of the resources is exhausted.
However the DefaultResourceCalculator only uses memory to control
the resources used by executors.
As a result sometimes you can see that the available of vcores on a node
become negative.
DominantResourceCalculator behaves the same way as workers in standalone mode.
It chooses the dominant resource as the upper limit for resource usage.

An infra perspective

I have heard that some companies use Kubernetes to spawn containers to host NodeManagers,
when there are huge demands for resources.

An ideal scenario is where the memory and cores in your cluster
are consumed at the same pace.
When memory is used up,
cores should be exhausted as well.
To achieve better utility of your Hadoop slaves,
tune these parameters such that
when memory and cores are used up by executors,
memory in OS is close to fully utilized and CPU load is slightly below
maximum capacity.
This advice is given in condition that Spark executors are the dominant
processes running on your servers
and you should always leave some memory and computational power for
other processes than Spark executors and tasks.
Your Spark applications might have very strange behavior,
sometimes even failures,
when the CPU load of Hadoop slaves is extremely high.

If you have both IO intensive (such as ETL)
and computationally intensive applications (such as data science apps),
consider introducing label-based scheduling before tuning.

Epilogue: data engineering in Shopee

As the primary data provider in a leading E-commerce platform operating across Southeast Asia,
data engineering team is able to handle TB-level in one ETL flow and
this number is still increasing fast.
We run Spark jobs on top of an in-house Hadoop cluster
whose size is among the top-tier in Singapore as well as SEA region, hopefully.
</content>
      <categories>
        
          <category> spark </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Offset topic and consumer group coordinator of Kafka</title>
      <url>/kafka/2016/11/04/Kafka-Group-Coordinator.html</url>
      <content type="text">This article is to discuss two subjects
that are not frequently or clearly covered by official document or
online sources.

Offset topic (the __consumer_offsets topic)

It is the only mysterious topic in Kafka log
and it cannot be deleted by using
TopicCommand.
Unfortunately there is no dedicated official documentation
to explain this internal topic.
The closest source I can find is
this set of slides.
I strongly recommend reading it if you wish to understand
how this internal topic works.

In short,
__consumer_offsets is used to store offsets of consumers
which was previously stored only in ZooKeeper before version 0.8.1.1.
At the latest version of 0.8.X serious, i.e. 0.8.2.2,
the storage location of offsets can be configured by
offsets.storage whose value can be either kafka or zookeeper.
If it is kafka,
consumers are still able to commit offsets to ZooKeeper
by enabling dual.commit.enabled.
However since version 0.9,
consumer offsets have been designed to be stored on brokers only.

The partition key of the messages in __consumer_offsets
was handled in
OffsetManager
at version 0.8.X
and has been named as OffsetKey in
GroupMetadataManager
since version 0.9.0.
It contains three pieces of information: groupId, topic and partition number,
and the key is serialized/de-serialized according to a schema called
OFFSET_COMMIT_KEY_SCHEMA.
The usage of schema is primarily for backward compatibility.
Unlike the behavior of
DefaultPartitioner,
the partition number inside __consumer_offsets is not
determined by the hash value of partition key
but only determined by the hash value of consumer group,
which is as simple as
Utils.abs(groupId.hashCode) % numPartitions

Here the numPartitions is configured by the value of
offsets.topic.num.partitions in broker configs
and is 50 by default.
This algorithm will be re-introduced later
in the part of consumer group coordination.

The values of offset messages which is named
OffsetMetadata
have two versions.
At version 0.8.X, the value contains offset, metadata (often empty)
and a timestamp
while the timestamp had been split into commit and expire timestamps
since version 0.9.0.
Console consumers are able to consume messages from internal topics
and print them out nicely.
A sample command has been shown below:

./kafka-simple-consumer-shell.sh --topic __consumer_offsets \
--partition 49 \
--broker-list localhost:9092 \
--formatter &quot;kafka.server.OffsetManager\$OffsetsMessageFormatter&quot;

By the way, absolute(&quot;testGroup&quot;.hashCode() % 50) = 49
which is the reason why partition 49 was specified.
It prints out:

[testGroup,testTopic-development,0]::OffsetAndMetadata[11,NO_METADATA,1478243992053]
[testGroup,testTopic-development,0]::OffsetAndMetadata[12,NO_METADATA,1478243992086]
[testGroup,testTopic-development,0]::OffsetAndMetadata[13,NO_METADATA,1478243992096]
[testGroup,testTopic-development,0]::OffsetAndMetadata[14,NO_METADATA,1478243992110]


However this is not the end of story,
because __consumer_offsets is also used by group coordinator
to store group metadata!
The following section discusses another new feature
introduced since version 0.9.0.

Group coordinator (coordinated rebalance)

This section is my humble and shallow understanding about
broker coordinator of consumer groups.
Correct me if I ever miss something or make any mistake.

The introduction of coordinator, according to the
official wiki of Kafka,
is to solve the split brain problem
which is a well known problem in a distributed system.
</content>
      <categories>
        
          <category> kafka </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Thoughts on Consumer Group of Apache Kafka</title>
      <url>/kafka/2016/10/25/Kafka-Consumer-Group.html</url>
      <content type="text">Introduction

The concept of consumer group is intriguring.
According to the
official documentation of Kafka,
one of its main objectives is to achieve message multicast and broadcast.
Often it is not an easy concept to grasp at the very beginning.
This article will share a few issues and discoveries about consumer group.

A good way to start understanding consumer group is
looking at its data structure within ZooKeeper,
which is illustrated below.



You may see each consumer group as a separated world in terms of message consumption.
Each consuemr group maintains its own consumer ID’s,
partition owners and offsets.

Rebalance

Consumer rebalance happens when consumers join or leave a consumer group or
when the topics within a consumer group have new partitions.

Consumer ID uniquely identitfies a consumer.
It is generated automatically when a consumer is launched.
The source code of its generation is defined in ZookeeperConsumerConnector
and shown below.

var consumerUuid : String = null
config.consumerId match {
  case Some(consumerId) // for testing only
  =&amp;gt; consumerUuid = consumerId
  case None // generate unique consumerId automatically
  =&amp;gt; val uuid = UUID.randomUUID()
  consumerUuid = &quot;%s-%d-%s&quot;.format(
    InetAddress.getLocalHost.getHostName, System.currentTimeMillis,
    uuid.getMostSignificantBits().toHexString.substring(0,8))
}
config.groupId + &quot;_&quot; + consumerUuid

If you go into /owners directory and check the owner of a partition,
you should find a very similar value to consumer ID.
The owner of a topic-partition is defined in the form of [consumerId]-[threadId] in which threadId is used
because one partition is meant to be consumed by exactly one thread.
In many cases, the threadId is equal to the partition number which
the consumer thread owns.

A consumer group is well balanced
if each partition inside it is owned by exactly one consumer thread.
You may use the instructions from this link
to verify the result of consumer rebalance.
This tool should work well because I have fixed a bug in VerifyConsumerRebalance.scala in this pull request.

Since version 0.9.0,
Kafka has used brokers to coordinate the rebalance process of consumers.
I also have thought about it and then wrote this blog.

Broadcast

Broadcast by Kafka is relatively cheap:
you just need to put each consumer in different consumer groups,
such that the offset of each consumer is different.

Once I read some articles online which suggest use UUID within
a consumer group, for example this question from stackoverflow and
it has received more than 5 upvotes.
Unfortunately I strongly discourage such usage.

Firstly and most importantly, consumer groups are stored as persistent nodes in ZooKeeper.
Often consumers need to be shutdown and started,
which may not be frequent in commercial environments
but is expected to be quite frequent in develop, test and staging phases.
As time goes by a huge number of consumer groups nodes
shall be accumulated in /consumers node and
most of them are not in use.
A direct result is
it is almost impossible to list out all the consumer groups.
ZooKeeper client throws runtime exception because the buffer overflows due to
the overwhelming amount of child nodes.
The error message is something like:

IOException Packet &amp;lt;len12343123123&amp;gt; is out of range


Even you configure the jute.maxbuffer parameter as some blogs suggest,
the chance of receiving a response
before your patience runs out is extremely low.
I have encountered a situation where /consumers has more than 90,000
child nodes and was unable to list up consumer groups ever since.
Another direct result is that some features of ConsumerGroupCommand
(used in kafka-consumer-groups.sh)
will fail because internally it lists out all consumer groups.

However, the huge number of consumer groups should not be a problem
for the normal operation of Kafka,
because the brokers do not need to know all the groups.
Even though from version 0.9 onward,
consumer rebalance is coordinated by brokers,
brokers do not scan groups when deciding which broker coordinates
which subset of them.
They only manage those consumer which join or leave them.
As far as what I have read from GroupCoordinator,
brokers do not scan all consumer groups.

Next, UUID is hardly queryable.
Its value is generated at runtime and unpredictable.
If you wish to describe a consumer group or check its offsets,
you will face problems on finding the exact value of it.
A workaround is to log its value somewhere.

Last but not the least,
group names with UUID in them is brand new every time.
Upon the creation of a new consumer group,
the value of offsets is determined by offset reset,
in other words the setting of auto.offset.reset.
No matter the value is smallest or largest,
the consumer either consumes many messages repeatedly or
skip some messages.

Delete a group

Deleting a consumer group from ZooKeeper is
as easy as one single instruction.
However, it implies that the offset information
will be permantly deleted from ZooKeeper.
Do it only when you are 100% sure that
the target group will never be reused.

From version 0.9.0 onward,
ConsumerGroupCommand
introduces a feature to remove consumer groups.
Nevertheless, it prohibits the removal of active consumer group by
checking the number of children under consuemr registry dicrectory
(i.e., /consumers/[group_name]/ids).
When you wish to remove a consumer group by a ZooKeeper client,
please take notice of active consumers as well.
</content>
      <categories>
        
          <category> kafka </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Advice for STM R&amp;D Assignments</title>
      <url>/programming/2014/12/28/STM-RD-Advice.html</url>
      <content type="text">Introduction &amp;amp; Disclaimer

STM (Starter Mission) is the training program for
new employees of
Works Applications Co., Ltd.
Each employee is  required to implement
several information systems.
Here are some practical guidelines for you
to program and deliver your systems more effectively.

Here we use a simple leave-application system as an illustration.
It has some basic requirements as below:


  There are two roles in the system: manager and employees.
  Managers should see applications of leaves from employees.
  Managers is able to approve or reject an application.
  The system may have other features such as departments.


But the main feature is the application of leaves.

Disclaimer:
this article does not provide solutions to any assignment in STM.
It never guarantees your success in STM.
The author is not responsible for any failure
because of taking the advice in this article.

Maintain a good quality of source code
Perhaps you have not programmed for months before STM and
you want to pass STM as soon as possible.
However these are not excuses to sacrifice your
code quality for faster development.
Believe me that you will probably spend more time
due to poor code quality.

Think about the five principles

You may have heard of the SOLID principles,
which are


  Single responsibility
  Open/closed
  Liskov substitution
  Interface segregation
  Dependency inversion


The first two principles helped me a lot
during the design of my own system.
Ask yourself when writing a class or method:


  Does it perform a single task for one responsibility?
  Is it closed for modification and open for extension?


For example,
combining adding and updating methods into one method called
apply() is not a good idea.
They had better be separated.
Another example is to implement public method cautiously.

Remember: if you strictly follow single responsibility principle,
open/closed principle would be natural for you as well.

Controller - the key part between view and model

Each controller should be responsible for only one feature.
It should not be defined by roles in the system.
One controller should be responsible for CRUD
(create, read, update and delete)
of exactly one feature, e.g., UserController.

In addition,
input validation and page redirection methods
should be separated from controllers as well.
Some people believe that the input validation logic should be bounded to each entity,
such that the data inside an entity is consistent wherever it is used.

Build up UI from components

Break down one page into different components and reuse them.
It saves you lots of time and greatly improves the maintainability of your UI.
This
simple tutorial will help you decompose your UI.

SQL builder (optional)

Write a SQL builder (wrapper) to generate SQL strings.
It does not save you lots of time,
but it improves maintainability of your SQL.

Dictionary generator (optional)

If you need to implement a dictionary feature,
consider writing a program to automatically generate dictionary insertion SQL.

What is a merit?

You need to be very clear on this principle since the beginning of STM:

Not every feature is a merit.

Some features are necessary for daily business operation.
Merits are always related to benefits.
They are those features that generate revenue,
improve efficiency,
enhance security,
ensure the stability of the system and operations.

Therefore when you write a catalog or implement a feature,
always ask yourself these questions:


  What merits does it bring to the user?
  What are the benefits of using my system compared to using just paper and pen?
  What makes my client want to buy my system?


Take the leave-application system for example:


  Does it really help the manager to maintain the leaves of his employees?
  Does it ease the process of applying and maintaining leaves?
  What is the advantage of my system over writing leaves on paper?


Is the UI intuitive to use?

Since you have stared at your system for a very long time,
you should be very familiar with it.
But the customer who use your system may not be.
Ask yourself these questions on each page (view):


  Is the user able to see the information presented in a clear and organized manner?
  Does the user know what he is doing and what to do next from the UI?
  Does the design and flow of my system effectively reduces clicks and page navigations?


If your answer is negative or not sure,
consider improving your UI.
The user should be able to use your system by
reading as few instructions as possible.

For example, consider using a
wizard
if there is a flow.
Give warning or confirmation before irrecoverable operations,
such as deletion and payment.
Do not purposefully choose fanciful (and complicated) UI component
just because it looks nice.
Simplicity has its value.

Robust testing using test cases

Your system is expected to be bug free
because nobody wants to buy a buggy product.

Write down test cases to systematically test your system.
Do not just randomly click around on the screen and
wish bugs will pop out by themselves.

If possible, automate the test, for example:
run unit test for Util classes.

Learn to use debug mode instead of printing messages into console.

Clear catalog: show how your system flows

My personal suggestion for your catalog is to
write use cases to demonstrate at least one complete flow of your system.

For example, your catalog should present the following work flow:
manager creates an employee profile -&amp;gt;
employee applies for a leave -&amp;gt;
manager approves that application -&amp;gt;
employee takes the leave -&amp;gt;
employee returns to work later than expected -&amp;gt;
manager updates the employee`s leave record

Summary

STM is not meant to fail you.
It is designed to equip you with
essential knowledge and skills for future work.
Please fully use the time and resources during STM and
it will surely benefit you in the days to come.
I wish you all the best.
</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Gallery Post</title>
      <url>/photo/2014/11/18/gallery-post.html</url>
      <content type="text">Nunc dignissim volutpat enim, non sollicitudin purus dignissim id. Nam sit amet urna eu velit lacinia eleifend. Proin auctor rhoncus ligula nec aliquet. Donec sodales molestie lacinia. Curabitur dictum faucibus urna at convallis. Aliquam in lectus at urna rutrum porta. In lacus arcu, molestie ut vestibulum ut, rhoncus sed eros. Sed et elit vitae risus pretium consectetur vel in mi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi tempus turpis quis lectus rhoncus adipiscing. Proin pulvinar placerat suscipit. Maecenas imperdiet, quam vitae varius auctor, enim mauris vulputate sapien, nec laoreet neque diam non quam.





Etiam luctus mauris at mi sollicitudin quis malesuada nibh porttitor. Vestibulum non dapibus magna. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Proin feugiat hendrerit viverra. Phasellus sit amet nunc mauris, eu ultricies tellus. Sed a mi tortor, eleifend varius erat. Proin consectetur molestie tortor eu gravida. Cras placerat orci id arcu tristique ut rutrum justo pulvinar. Maecenas lacinia fringilla diam non bibendum. Aenean vel viverra turpis. Integer ut leo nisi. Pellentesque vehicula quam ut sapien convallis consequat. Aliquam ut arcu purus, eget tempor purus. Integer eu tellus quis erat tristique gravida eu vel lorem.
</content>
      <categories>
        
          <category> Photo </category>
        
      </categories>
      <tags>
        
          <tag> consectetur </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>It has been two years</title>
      <url>/life/2014/02/03/TwoYears.html</url>
      <content type="text">It has been two years since I joined github.



Since then my life has stepped into AD (After Development) from
BC (Before Coding).
</content>
      <categories>
        
          <category> life </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Ocaml React package tutorial</title>
      <url>/programming/2014/01/28/React-Tutorial.html</url>
      <content type="text">React is an Ocaml package made for reactive programming
using Ocaml.
The objective of this post is to illustrate and explain some examples of the React API.
Therefore the basic concepts of React, such as Signals, Events and side effects, will not be repeated here.
Below are some prerequisites for reading this post.


  Ocaml (version 4.00.1 or above)
  OPAM (Ocaml PAckage Manager)
  React (installation via OPAM)
  React Introduction


Most of the examples used in this post are inspired by
test cases
of React project.
To report an issue in the post,
contact me through my Github account.
Thank you for your feedbacks.

Using React package
For compilation

ocamlfind ocamlopt -o clock -linkpkg \
    -package react clock.ml


For Ocaml Toplevel

#use &quot;topfind&quot;;;
#require &quot;react&quot;;;
open React;;


Module React.E

val once : 'a React.event -&amp;gt; 'a React.event

once e is e with only its next occurence.
Any occurence after time t will not affect e.

1
2
3
4
5
open React (* omitted in the examples below *)
let w, send_w = E.create ()
let x = E.once w
let pr_x = E.map print_int x
let () = List.iter send_w [0; 1; 2; 3]


The output would be only 0.
All the values after the first occurence are omitted.

val drop_once : 'a React.event -&amp;gt; 'a React.event

drop_once e is e without its next occurrence.
Similarly, if we change the third line of the example above,
the output would be 123.
The reason is because the NEXT occurrence of e is omitted.

val app : ('a -&amp;gt; 'b) React.event -&amp;gt; 'a React.event -&amp;gt; 'b React.event

app ef e occurs when both ef and e occur simultaneously.

let f x y = x + y
let w, send_w = E.create ()
let x = E.map (fun w -&amp;gt; f w) w
let y = E.drop_once w
let z = E.app x y
let pr_z = E.map print_int z
List.iter send_w [0; 1; 2; 3]

The output would be 246.
As explained before,
drop_once makes y only take updates from the second occurrence onwards.
Variable z takes the value of x when both x and y are updated.

val map : ('a -&amp;gt; 'b) -&amp;gt; 'a React.event -&amp;gt; 'b React.event

map f e applies f to e’s occurrences.

val stamp : 'b React.event -&amp;gt; 'a -&amp;gt; 'a React.event

stamp e v is map (fun _ -&amp;gt; v) e.
It fixes the occurrence of e to v.

val filter : ('a -&amp;gt; bool) -&amp;gt; 'a React.event -&amp;gt; 'a React.event

filter p e are e’s occurrences that satisfy p.

val fmap : ('a -&amp;gt; 'b option) -&amp;gt; 'a React.event -&amp;gt; 'b React.event

fmap fm e are occurrences of e filtered and mapped by fm.

let v, send_v = E.create ()
let x = E.stamp v 1 (* x: 1111 *)
let y = E.filter (fun n -&amp;gt; n mod 2 = 0) v (* y: 02 *)
let z = E.fmap (fun n -&amp;gt; if n = 0 then Some 1 else None) v (* z: 1 *)
List.iter send_v [0; 1; 2; 3]

val diff : ('a -&amp;gt; 'a -&amp;gt; 'b) -&amp;gt; 'a React.event -&amp;gt; 'b React.event

diff f e occurs whenever e occurs except on the next occurence.
In another words,
it is triggered since the second the occurrence.
Occurences are f v v’ where v is e’s current occurrence and v’ the previous one.

val changes : ?eq:('a -&amp;gt; 'a -&amp;gt; bool) -&amp;gt; 'a React.event -&amp;gt; 'a React.event

changes eq e is the occurrences of e with occurences equal to the previous one dropped.
Equality is tested with eq (defaults to structural equality).
The behavior is similar to Signals.

let x, send_x = E.create ()
let y = E.diff ( - ) x
let z = E.changes x
List.iter send_x [0; 0; 3; 4; 4]

The occurrences of y are 0310 (difference between current and previous one),
while those of z are 034 (only changes are recorded).

val dismiss : 'b React.event -&amp;gt; 'a React.event -&amp;gt; 'a React.event

dismiss c e is the occurences of e except the ones when c occurs.

let x, send_x = E.create ()
let y = E.fmap (fun x -&amp;gt; if x mod 2 = 0 then Some x else None) x
let z = E.dismiss y x
List.iter send_x [1; 2; 3; 4; 5; 6]

The occurrences of y are 246 (even numbers).
Occurrences of z are 135.
When y has occurrence, z will not have any occurrence.

val until : 'a React.event -&amp;gt; 'b React.event -&amp;gt; 'b React.event

until c e is e’s occurences until c occurs.

let x, send_x = E.create ()
let stop = E.filter (fun s -&amp;gt; s = &quot;c&quot;) x
let y = E.until stop x
let pr_y = E.map print_string y
List.iter send_x [&quot;a&quot;; &quot;b&quot;; &quot;c&quot;; &quot;d&quot;; &quot;e&quot;]

The output is ab.
All the occurrences after stop event are dropped,
although stop has no more new updates.

val accum : ('a -&amp;gt; 'a) React.event -&amp;gt; 'a -&amp;gt; 'a React.event

accum ef i accumulates a value, starting with i,
using e’s functional occurrences.
It takes the previous occurrence, applies to the function and evaluates
the result as the current occurrence.

let f, send_f = E.create ()
let a = E.accum f 0
let pr_a = E.map print_int a
List.iter send_f [( + ) 2; ( - ) 1; ( * ) 2];
(* Output is 2 -1 -2 *)

val fold : ('a -&amp;gt; 'b -&amp;gt; 'a) -&amp;gt; 'a -&amp;gt; 'b React.event -&amp;gt; 'a React.event

fold f i e accumulates the occurrences of e with f starting with i.
The behavior of fold is similar to List.fold_left.

let x, send_x = E.create ()
let y = E.fold ( * ) 1 x
let pr_y = E.map print_int y
List.iter send_x [1; 2; 3; 4]

The output would be 1 2 6 24.

val select : 'a React.event list -&amp;gt; 'a React.event

select el is the occurrences of every event in el.
If more than one event occurs simultaneously the leftmost is taken and the others are lost.

let w, send_w = E.create ()
let x, send_x = E.create ()
let y = E.map succ w
let z = E.map succ y (* z and y are considered simultaneous *)
let t = E.select [w; x] (* t: 0 1 2 3 4 *)
let sy = E.select [y; z] (* sy: 1 3 4 5 *)
let sz = E.select [z; y] (* sz: 2 4 5 6 *)
send_w 0;
send_x 1;
List.iter send_w [2; 3; 4]

The expected occurrences of all variables are written in the comments.
To know more about simultaneous events, read
this.

val merge : ('a -&amp;gt; 'b -&amp;gt; 'a) -&amp;gt; 'a -&amp;gt; 'b React.event list -&amp;gt; 'a React.event

merge f a el merges the simultaneous occurrences of every event in el using f and the accumulator a.
You may refer to the previous API to check the meaning of simultaneous events.

let w, send_w = E.create ()
let x, send_x = E.create ()
let y = E.map succ w (* y is a simultaneous event w.r.t. w *)
let z = E.merge (fun acc v -&amp;gt; v::acc) [] [w; x; y]
send_w 1;
send_x 4;
send_w 2;

The occurrences of z are [2; 1], [4], [3; 2].
Hint: fun acc v -&amp;gt; v::acc pushes every v to the accumulator acc.

val switch : 'a React.event -&amp;gt; 'a React.event React.event -&amp;gt; 'a React.event

switch e ee is e’s occurrences until there is an occurrence e’ on ee,
the occurrences of e’ are then used until there is a new occurrence on ee, etc.

val fix : ('a React.event -&amp;gt; 'a React.event * 'b) -&amp;gt; 'b

fix ef allows to refer to the value an event had an infinitesimal amount of time before.

Module React.S

val hold : ?eq:('a -&amp;gt; 'a -&amp;gt; bool) -&amp;gt; 'a -&amp;gt; 'a React.event -&amp;gt; 'a React.signal

hold i e has the value of e’s last occurrence or i if there was not any.

1
2
3
4
let x, send_x = E.create ()
let a = S.hold 0 x
let pr_a = S.map print_int a
List.iter send_x [1; 2; 3; 3; 3; 4; 4; 4]


A 0 will printed out at line 3 because x has no occurrences by then.
Then 1234 will be printed.
Note that only updates that change the value of a signal are printed.

Many API are similar to those in React.E.
Therefore this tutorial jumps straight to Lifting combinators.
Lifting transforms a regular function to make it act on signals.
For examples, l2 takes a function with two arguments as its input.

val l2 : ?eq:('c -&amp;gt; 'c -&amp;gt; bool) -&amp;gt;
    ('a -&amp;gt; 'b -&amp;gt; 'c) -&amp;gt; 'a React.signal -&amp;gt; 'b React.signal -&amp;gt; 'c React.signal

Example:

let f x y = x mod y
let fl = S.l2 f
val fl : int React.signal -&amp;gt; int React.signal -&amp;gt; int React.signal = &amp;lt;fun&amp;gt;

Some other examples


  Clock.ml
  Breakout.ml
  Oreo project Ocaml Reactive programming used on Web authored by me





</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Differences between type systems of OCaml &amp; Java</title>
      <url>/programming/2013/12/27/Type-Systems.html</url>
      <content type="text">Introduction

When asked the differences between the OCaml’s type system and Java’s,
I realized that I did not understand them well.
Therefore, I did some research about these two type systems and
posted the result as this blog.

OCaml Type System

The ML languages are statically and strictly typed.
In addition, every expression has exactly one type.
In contrast, C is a weakly-typed language:
values of one type can usually coerced to a value of any other type,
whether the coercion makes sense or not.

What is “safety”?
There is a formal definition based on the operational semantics of
the programming language,
but an approximate definition is that a valid program will never fault
because of an invalid machine operation.
All memory access will be valid.
ML guarantees safety by proving that every program that
passes the type checker can never produce a machine fault.
Another functional language,
Lisp which is dynamically and strictly typed,
guarantees its safety by checking for validity at run time.
One of the consequences is that ML has no nil or NULL values.
These values would potentially cause machine errors if used
where a value is expected.

An OCaml programmer may initially spend a lot of time getting the OCaml
type checker to accept his programs.
But eventually
he will find the type checker is one of his best friends.
It helps a programmer figure out
where errors may be lurking in programs.
If a change is made, the type checker will track down the parts that
are affected.
At the meantime,
here are some rules about type checking.


  Every expression has exactly one type.
  When an expression is evaluated, one of the four things may happen:
    
      it may evaluate to a value of the same type as the expression,
      it may raise an exception,
      it may not terminate,
      it may exit.
    
  


One of the important points in OCaml is that
there are no “pure commands”.
Every assignment produce a value

  although the value has the trivial unit type.
Below let us see some examples of how OCaml type system works:


1
2
3
4
if 1 &amp;lt; 2 then
  1
else
  1.3


When compiling the code,
the compiler would tell you there is a type error on line 4,
characters 3-6. This is because the type checker requires that
both branches of the conditional statement have the same type
no matter how the test turns out.
Since the expressions 1 and 1.3 have different types,
the type checker generates an error.

A Strong Static Language: Java

Java is considered one of the most static languages,
but it implemented a comprehensive reflection API which
allows you to change classes at run time,
thus resembling more dynamic languages.
This feature enables Java Virtue Machine (JVM)
to support very dynamic languages such as Groovy.

Java needs everything to be defined so that
you know all the time what type of object you have and
whether you call them properly.
In addition,
Java does not allow code outside of a class.
It has been a major reason why people complain that
Java forces you to write too much boilerplate.

The popularity of Java and its strong adherence to strong typing
made a huge impact on the programming landscape.
Strong typing advocates lauded Java for fixing the cracks in C++.
But many programmers found Java overly prescriptive and rigid.
They wanted a fast way to write code without all of the extra definition of Java.
As a result, strong dynamic typing languages,
such as JavaScript, Python and Ruby, are becoming more and more popular
in recent years.

What are their differences?

Anyway,
there is no stronger or weaker type system between these two systems.
OCaml and Java are both static and strongly typed languages.
They both have primitive, array and class types.
(The standard ML language does not support class types but
Objective-caml does.)
However Java and OCaml do have some differences in their type systems.
OCaml is a functional language and
a function can be passed as a argument,
known as higher order functions.
The type of a function is defined by both its input and output type.
For example in the chunck of code below,

1
2
let square x = x * x;
val square : int -&amp;gt; int = &amp;lt;fun&amp;gt;


square is a function that takes an integer and returns an integer.
Most importantly,
square can be passed to another function as an input.
However it is not done as natural in Java.
In Java higher order function is usually done by
wrapping the function within an interface as below:

1
2
3
4
5
6
7
public int methodToPass() {
	// do something
}

public void dansMethod(int i, Callable&amp;lt;Integer&amp;gt; myFunc) {
	// do something
}


then use a inner class to call the method methodToPass as shown below:

1
2
3
4
5
dansMethod(100, new Callable&amp;lt;Integer&amp;gt;() {
	public Integer call() {
		return methodToPass();
	}
});


Next, pattern matching is something unique to OCaml compared to Java.
A similar feature in Java is perhaps switch statement.
However it does not have many things to do with type system.

1
2
3
4
let rec sigma f = function
	| [] -&amp;gt; 0
	| x :: l -&amp;gt; f x + sigma f l;;
val sigma : ('a -&amp;gt; int) -&amp;gt; 'a list -&amp;gt; int = &amp;lt;fun&amp;gt;


Last but not the least,
type inference is a major difference between OCaml and Java type systems.
Most conventional static typing languages require programmers to restate the types of expressions.
For example,
the following Java code instantiates a method to increment an integer by 1.

1
2
3
int succ (int n) {
    return n + 1;
}


In OCaml, all programs can be written such that the types they use are completely inferred, i.e.
it is never necessary to explicitly define and declare types in OCaml.
However, defining important types in a program is a good way to leverage static type checking by providing machine-checked documentation,
improving error reporting and tightening the type system to catch more errors.

The above Java code can be re-written as

1
2
let succ n = n + 1;;
val succ : int -&amp;gt; int = &amp;lt;fun&amp;gt;


OCaml infers that the function maps integers onto integers even though no types were declared.
The inference was based upon the use of the integer operator + and the integer literal 1.

Conclusion

In summary,
the type system of Java has lots of similarities compared to OCaml.
Still Java is one of the dominant and most popular programming languages.
</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Welcome to Jekyll</title>
      <url>/programming/2013/07/17/welcome-to-jekyll.html</url>
      <content type="text">You’ll find this post in your _posts directory - edit this post and re-build (or run with the -w switch) to see your changes!
To add new posts, simply add a file in the _posts directory that follows the convention: YYYY-MM-DD-name-of-post.ext.

Jekyll also offers powerful support for code snippets:

def print_hi(name)
  puts &quot;Hi, #{name}&quot;
end
print_hi('Tom')
#=&amp;gt; prints 'Hi, Tom' to STDOUT.

Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo.

</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Bootstrap, bootstrap everywhere</title>
      <url>/programming/2013/06/19/Bootstrap-Everywhere.html</url>
      <content type="text">In layman’s terms,
bootstrap is a CSS and Javascript framework designed to
make web development easier.
At the same time, it makes the websites look similar.
If the framework is used without any customization,
the produced websites will look almost in the same style.



Anyway, I do not say twitter bootstrap is bad.
Even this blog uses this framework.
Imagine if I built the CSS from scratch, perhaps I would have not finished
writing it.

Appended on 10-Jan-2014:

Since the new release of the website,
the CSS of this website has given up bootstrap framework.
</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title>Why I used Jekyll-bootstrap instead of Octopress</title>
      <url>/programming/2013/06/13/Why-I-Used-Jekyll-Instead-of-Octopress.html</url>
      <content type="text">I was amazed by the beautiful blog made by my friend.
He suggested me use Octopress or Jekyll.
However I faced a lot of problems when I tried to install both of them.

Below are some examples:

Problem 1

Building native extensions.  This could take a while...
ERROR:  Error installing jekyll:
ERROR: Failed to build gem native extension.

/usr/bin/ruby1.9.1 extconf.rb
/usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- mkmf (LoadError)
from /usr/lib/ruby/1.9.1/rubygems/custom_require.rb:36:in `require'
from extconf.rb:1:in `&amp;lt;main&amp;gt;'


Solution 1

sudo apt-get install ruby1.9.1-dev


Problem 2

Bundler::GemfileNotFound


Solution 2
find the Gemfile by command:

locate Gemfile


cd into the directory

Problem 3

rake aborted!
You have already activated rake 0.9.6, but your Gemfile requires rake 0.9.2.2.


Solution 3
run command

$ bundle exec rake install


Apply to all the incidents when rake is aborted.

Finally the big boss came and I was unable to configure the rake such that it can
point to my github local repo. Sad…

Now I switch to Jekyll Bootstrap. Hopefully it is a better solution.
Hopefully it was not because I was noob.
</content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
